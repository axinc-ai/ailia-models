import os
import sys
import math
import random
import cv2
import numpy as np
from PIL import Image, ImageDraw, ImageFont

from scipy.special import softmax

from craft_utils import getDetBoxes, adjustResultCoordinates
from config.model import recognition_models


# ======================
# PARAMETERS
# ======================
# for detection
canvas_size = 2560
mag_ratio = 1.
text_threshold = 0.7
link_threshold = 0.4
low_text = 0.4
poly = False
optimal_num_chars = False
estimate_num_chars = None
min_size = 20
slope_ths = 0.1
ycenter_ths = 0.5
height_ths = 0.5
width_ths = 0.5 #width_ths = 1.0
add_margin = 0.1 #add_margin = 0.5

# for recognition
imgH = 64
batch_size = 1
contrast_ths = 0.1
adjust_contrast = 0.5
filter_ths = 0.003
workers = 0 #workers = 1
#y_ths = 0.5
#x_ths = 1.0
#beamWidth= 5
#allowlist = None
#blocklist = None
#detail = 1
#rotation_info = None
#paragraph = False


# ======================
# DETECTION
# ======================
def diff(input_list):
    return max(input_list)-min(input_list)


def normalizeMeanVariance(in_img, mean=(0.485, 0.456, 0.406), variance=(0.229, 0.224, 0.225)):
    # should be RGB order
    img = in_img.copy().astype(np.float32)

    img -= np.array([mean[0] * 255.0, mean[1] * 255.0, mean[2] * 255.0], dtype=np.float32)
    img /= np.array([variance[0] * 255.0, variance[1] * 255.0, variance[2] * 255.0], dtype=np.float32)
    return img


def resize_aspect_ratio(img, square_size, interpolation):
    height, width, channel = img.shape

    # magnify image size
    target_size = mag_ratio * max(height, width)

    # set original image size
    if target_size > square_size:
        target_size = square_size

    ratio = target_size / max(height, width)

    target_h, target_w = int(height * ratio), int(width * ratio)
    proc = cv2.resize(img, (target_w, target_h), interpolation = interpolation)


    # make canvas and paste image
    target_h32, target_w32 = target_h, target_w
    if target_h % 32 != 0:
        target_h32 = target_h + (32 - target_h % 32)
    if target_w % 32 != 0:
        target_w32 = target_w + (32 - target_w % 32)
    resized = np.zeros((target_h32, target_w32, channel), dtype=np.float32)
    resized[0:target_h, 0:target_w, :] = proc
    target_h, target_w = target_h32, target_w32

    size_heatmap = (int(target_w/2), int(target_h/2))

    return resized, ratio, size_heatmap


def detect(net, image):

    # batch format
    image_arrs = [image]

    # resize
    img_resized_list = []
    for img in image_arrs:
        img_resized, target_ratio, size_heatmap = resize_aspect_ratio(img, canvas_size, cv2.INTER_LINEAR)
        img_resized_list.append(img_resized)
    ratio_h = ratio_w = 1 / target_ratio

    # preprocessing
    x = [np.transpose(normalizeMeanVariance(n_img), (2, 0, 1)) for n_img in img_resized_list]
    x = np.array(x)

    # predict
    y = net.predict(x) # y, feature = net(x)

    # postprocessing
    boxes_list, polys_list = [], []
    for out in y:
        # make score and link map
        score_text = out[:, :, 0]
        score_link = out[:, :, 1]

        # Post-processing
        boxes, polys, mapper = getDetBoxes(score_text, score_link, text_threshold, link_threshold, low_text, poly, estimate_num_chars)

        # coordinate adjustment
        boxes = adjustResultCoordinates(boxes, ratio_w, ratio_h)
        polys = adjustResultCoordinates(polys, ratio_w, ratio_h)
        if estimate_num_chars:
            boxes = list(boxes)
            polys = list(polys)
        for k in range(len(polys)):
            if estimate_num_chars:
                boxes[k] = (boxes[k], mapper[k])
            if polys[k] is None:
                polys[k] = boxes[k]
        boxes_list.append(boxes)
        polys_list.append(polys)

    return boxes_list, polys_list


def group_text_box(polys, sort_output = True):
    # poly top-left, top-right, low-right, low-left
    horizontal_list, free_list, combined_list, merged_list = [],[],[],[]

    for poly in polys:
        slope_up = (poly[3]-poly[1])/np.maximum(10, (poly[2]-poly[0]))
        slope_down = (poly[5]-poly[7])/np.maximum(10, (poly[4]-poly[6]))
        if max(abs(slope_up), abs(slope_down)) < slope_ths:
            x_max = max([poly[0],poly[2],poly[4],poly[6]])
            x_min = min([poly[0],poly[2],poly[4],poly[6]])
            y_max = max([poly[1],poly[3],poly[5],poly[7]])
            y_min = min([poly[1],poly[3],poly[5],poly[7]])
            horizontal_list.append([x_min, x_max, y_min, y_max, 0.5*(y_min+y_max), y_max-y_min])
        else:
            height = np.linalg.norm([poly[6]-poly[0],poly[7]-poly[1]])
            width = np.linalg.norm([poly[2]-poly[0],poly[3]-poly[1]])

            margin = int(1.44*add_margin*min(width, height))

            theta13 = abs(np.arctan( (poly[1]-poly[5])/np.maximum(10, (poly[0]-poly[4]))))
            theta24 = abs(np.arctan( (poly[3]-poly[7])/np.maximum(10, (poly[2]-poly[6]))))
            # do I need to clip minimum, maximum value here?
            x1 = poly[0] - np.cos(theta13)*margin
            y1 = poly[1] - np.sin(theta13)*margin
            x2 = poly[2] + np.cos(theta24)*margin
            y2 = poly[3] - np.sin(theta24)*margin
            x3 = poly[4] + np.cos(theta13)*margin
            y3 = poly[5] + np.sin(theta13)*margin
            x4 = poly[6] - np.cos(theta24)*margin
            y4 = poly[7] + np.sin(theta24)*margin

            free_list.append([[x1,y1],[x2,y2],[x3,y3],[x4,y4]])
    if sort_output:
        horizontal_list = sorted(horizontal_list, key=lambda item: item[4])

    # combine box
    new_box = []
    for poly in horizontal_list:

        if len(new_box) == 0:
            b_height = [poly[5]]
            b_ycenter = [poly[4]]
            new_box.append(poly)
        else:
            # comparable height and comparable y_center level up to ths*height
            if abs(np.mean(b_ycenter) - poly[4]) < ycenter_ths*np.mean(b_height):
                b_height.append(poly[5])
                b_ycenter.append(poly[4])
                new_box.append(poly)
            else:
                b_height = [poly[5]]
                b_ycenter = [poly[4]]
                combined_list.append(new_box)
                new_box = [poly]
    combined_list.append(new_box)

    # merge list use sort again
    for boxes in combined_list:
        if len(boxes) == 1: # one box per line
            box = boxes[0]
            margin = int(add_margin*min(box[1]-box[0],box[5]))
            merged_list.append([box[0]-margin,box[1]+margin,box[2]-margin,box[3]+margin])
        else: # multiple boxes per line
            boxes = sorted(boxes, key=lambda item: item[0])

            merged_box, new_box = [],[]
            for box in boxes:
                if len(new_box) == 0:
                    b_height = [box[5]]
                    x_max = box[1]
                    new_box.append(box)
                else:
                    if (abs(np.mean(b_height) - box[5]) < height_ths*np.mean(b_height)) and ((box[0]-x_max) < width_ths *(box[3]-box[2])): # merge boxes
                        b_height.append(box[5])
                        x_max = box[1]
                        new_box.append(box)
                    else:
                        b_height = [box[5]]
                        x_max = box[1]
                        merged_box.append(new_box)
                        new_box = [box]
            if len(new_box) >0: merged_box.append(new_box)

            for mbox in merged_box:
                if len(mbox) != 1: # adjacent box in same line
                    # do I need to add margin here?
                    x_min = min(mbox, key=lambda x: x[0])[0]
                    x_max = max(mbox, key=lambda x: x[1])[1]
                    y_min = min(mbox, key=lambda x: x[2])[2]
                    y_max = max(mbox, key=lambda x: x[3])[3]

                    box_width = x_max - x_min
                    box_height = y_max - y_min
                    margin = int(add_margin * (min(box_width, box_height)))

                    merged_list.append([x_min-margin, x_max+margin, y_min-margin, y_max+margin])
                else: # non adjacent box in same line
                    box = mbox[0]

                    box_width = box[1] - box[0]
                    box_height = box[3] - box[2]
                    margin = int(add_margin * (min(box_width, box_height)))

                    merged_list.append([box[0]-margin,box[1]+margin,box[2]-margin,box[3]+margin])
    # may need to check if box is really in image
    return merged_list, free_list


def detector_predict(net, image):
    # get textbox
    bboxes_list, polys_list = detect(net, image)
    result = []
    for polys in polys_list:
        single_img_result = []
        for i, box in enumerate(polys):
            poly = np.array(box).astype(np.int32).reshape((-1))
            single_img_result.append(poly)
        result.append(single_img_result)
    text_box_list = result

    # convert bbox [x1, x2, y1, y2]
    horizontal_list_agg, free_list_agg = [], []
    for text_box in text_box_list:
        horizontal_list, free_list = group_text_box(text_box, (optimal_num_chars is None))
        if min_size:
            horizontal_list = [i for i in horizontal_list if max(i[1] - i[0], i[3] - i[2]) > min_size]
            free_list = [i for i in free_list if max(diff([c[0] for c in i]), diff([c[1] for c in i])) > min_size]
        horizontal_list_agg.append(horizontal_list)
        free_list_agg.append(free_list)

    return horizontal_list_agg, free_list_agg


# ======================
# RECOGNITION
# ======================
class CTCLabelConverter(object):
    """ Convert between text-label and text-index """

    def __init__(self, character, separator_list = {}, dict_pathlist = {}):
        # character (str): set of the possible characters.
        dict_character = list(character)

        self.dict = {}
        for i, char in enumerate(dict_character):
            self.dict[char] = i + 1

        self.character = ['[blank]'] + dict_character  # dummy '[blank]' token for CTCLoss (index 0)

        self.separator_list = separator_list
        separator_char = []
        for lang, sep in separator_list.items():
            separator_char += sep
        self.ignore_idx = [0] + [i+1 for i,item in enumerate(separator_char)]

        ####### latin dict
        if len(separator_list) == 0:
            dict_list = []
            for lang, dict_path in dict_pathlist.items():
                try:
                    with open(dict_path, "r", encoding = "utf-8-sig") as input_file:
                        word_count =  input_file.read().splitlines()
                    dict_list += word_count
                except:
                    pass
        else:
            dict_list = {}
            for lang, dict_path in dict_pathlist.items():
                with open(dict_path, "r", encoding = "utf-8-sig") as input_file:
                    word_count =  input_file.read().splitlines()
                dict_list[lang] = word_count

        self.dict_list = dict_list

    def decode_greedy(self, text_index, length):
        """ convert text-index into text-label. """
        texts = []
        index = 0
        for l in length:
            t = text_index[index:index + l]
            # Returns a boolean array where true is when the value is not repeated
            a = np.insert(~((t[1:]==t[:-1])),0,True)
            # Returns a boolean array where true is when the value is not in the ignore_idx list
            b = ~np.isin(t,np.array(self.ignore_idx))
            # Combine the two boolean array
            c = a & b
            # Gets the corresponding character according to the saved indexes
            text = ''.join(np.array(self.character)[t[c.nonzero()]])
            texts.append(text)
            index += l
        return texts


class NormalizePAD(object):

    def __init__(self, max_size, PAD_type='right'):
        self.max_size = max_size
        self.max_width_half = math.floor(max_size[2] / 2)
        self.PAD_type = PAD_type

    def __call__(self, img):
        img = np.array(img)/255
        img = img[np.newaxis, :, :]
        img = (img-0.5)/0.5
        c, h, w = img.shape

        Pad_img = np.zeros(self.max_size)
        Pad_img[:, :, :w] = img  # right pad
        if self.max_size[2] != w:  # add border Pad
            tmp = img[:, :, w - 1]
            tmp = tmp[:, :, np.newaxis]
            Pad_img[:, :, w:] = tmp.repeat(self.max_size[2] - w, axis=2)

        return Pad_img


def preprocess(img, imgH, imgW, keep_ratio_with_pad=False, adjust_contrast = 0.):

    img = Image.fromarray(img, 'L')
    images = [img]
    resized_max_w = imgW
    input_channel = 1
    transform = NormalizePAD((input_channel, imgH, resized_max_w))

    resized_images = []
    for image in images:
        w, h = image.size
        #### augmentation here - change contrast
        if adjust_contrast > 0:
            image = np.array(image.convert("L"))
            image = adjust_contrast_grey(image, target = adjust_contrast)
            image = Image.fromarray(image, 'L')

        ratio = w / float(h)
        if math.ceil(imgH * ratio) > imgW:
            resized_w = imgW
        else:
            resized_w = math.ceil(imgH * ratio)

        resized_image = image.resize((resized_w, imgH), Image.BICUBIC)
        resized_images.append(transform(resized_image))

    image_tensors = np.concatenate([t[np.newaxis, :, :, :] for t in resized_images], 0)
    return image_tensors


def custom_mean(x):
    return x.prod()**(2.0/np.sqrt(len(x)))


def calculate_ratio(width, height):
    '''
    Calculate aspect ratio for normal use case (w>h) and vertical text (h>w)
    '''
    ratio = width/height
    if ratio<1.0:
        ratio = 1./ratio
    return ratio


def compute_ratio_and_resize(img, width, height, model_height):
    '''
    Calculate ratio and resize correctly for both horizontal text
    and vertical case
    '''
    ratio = width/height
    if ratio<1.0:
        ratio = calculate_ratio(width,height)
        img = cv2.resize(img,(model_height,int(model_height*ratio)), interpolation=Image.LANCZOS)
    else:
        img = cv2.resize(img,(int(model_height*ratio),model_height),interpolation=Image.LANCZOS)
    return img,ratio


def contrast_grey(img):
    high = np.percentile(img, 90)
    low  = np.percentile(img, 10)
    return (high-low)/np.maximum(10, high+low), high, low


def adjust_contrast_grey(img, target = 0.4):
    contrast, high, low = contrast_grey(img)
    if contrast < target:
        img = img.astype(int)
        ratio = 200./np.maximum(10, high-low)
        img = (img - low + 25)*ratio
        img = np.maximum(np.full(img.shape, 0) ,np.minimum(np.full(img.shape, 255), img)).astype(np.uint8)
    return img


def four_point_transform(image, rect):
    (tl, tr, br, bl) = rect

    widthA = np.sqrt(((br[0] - bl[0]) ** 2) + ((br[1] - bl[1]) ** 2))
    widthB = np.sqrt(((tr[0] - tl[0]) ** 2) + ((tr[1] - tl[1]) ** 2))
    maxWidth = max(int(widthA), int(widthB))

    # compute the height of the new image, which will be the
    # maximum distance between the top-right and bottom-right
    # y-coordinates or the top-left and bottom-left y-coordinates
    heightA = np.sqrt(((tr[0] - br[0]) ** 2) + ((tr[1] - br[1]) ** 2))
    heightB = np.sqrt(((tl[0] - bl[0]) ** 2) + ((tl[1] - bl[1]) ** 2))
    maxHeight = max(int(heightA), int(heightB))

    dst = np.array([[0, 0],[maxWidth - 1, 0],[maxWidth - 1, maxHeight - 1],[0, maxHeight - 1]], dtype = "float32")

    # compute the perspective transform matrix and then apply it
    M = cv2.getPerspectiveTransform(rect, dst)
    warped = cv2.warpPerspective(image, M, (maxWidth, maxHeight))

    return warped


def set_language_list(lang_list, symbol):
    lang_char = []
    for lang in lang_list:
        char_file = os.path.join('config', lang + "_char.txt")
        with open(char_file, "r", encoding = "utf-8-sig") as input_file:
            char_list =  input_file.read().splitlines()
        lang_char += char_list
    lang_char = set(lang_char).union(set(symbol))
    lang_char = ''.join(lang_char)
    return lang_char


def get_image_list(horizontal_list, free_list, img, model_height = 64, sort_output = True):
    image_list = []
    maximum_y,maximum_x = img.shape

    max_ratio_hori, max_ratio_free = 1,1
    for box in free_list:
        rect = np.array(box, dtype = "float32")
        transformed_img = four_point_transform(img, rect)
        ratio = calculate_ratio(transformed_img.shape[1],transformed_img.shape[0])
        new_width = int(model_height*ratio)
        if new_width == 0:
            pass
        else:
            crop_img,ratio = compute_ratio_and_resize(transformed_img,transformed_img.shape[1],transformed_img.shape[0],model_height)
            image_list.append( (box,crop_img) ) # box = [[x1,y1],[x2,y2],[x3,y3],[x4,y4]]
            max_ratio_free = max(ratio, max_ratio_free)

    max_ratio_free = math.ceil(max_ratio_free)

    for box in horizontal_list:
        x_min = max(0,box[0])
        x_max = min(box[1],maximum_x)
        y_min = max(0,box[2])
        y_max = min(box[3],maximum_y)
        crop_img = img[y_min : y_max, x_min:x_max]
        width = x_max - x_min
        height = y_max - y_min
        ratio = calculate_ratio(width,height)
        new_width = int(model_height*ratio)
        if new_width == 0:
            pass
        else:
            crop_img,ratio = compute_ratio_and_resize(crop_img,width,height,model_height)
            image_list.append( ( [[x_min,y_min],[x_max,y_min],[x_max,y_max],[x_min,y_max]] ,crop_img) )
            max_ratio_hori = max(ratio, max_ratio_hori)

    max_ratio_hori = math.ceil(max_ratio_hori)
    max_ratio = max(max_ratio_hori, max_ratio_free)
    max_width = math.ceil(max_ratio)*model_height

    if sort_output:
        image_list = sorted(image_list, key=lambda item: item[0][0][1]) # sort by vertical position
    return image_list, max_width


def recognize(language, net, converter, img_list, batch_max_length, ignore_idx):
    result = []
    for t, image_tensors in enumerate(img_list):
        batch_size = image_tensors.shape[0]
        image = image_tensors
        tmp = image.squeeze()

        #
        # Set appropriate constant width because current onnx exporter does not support operator adaptive pooling with dynamic axis.
        #
        # ONNX export of operator adaptive pooling, since output_size is not constant..
        # Please feel free to request support or submit a pull request on PyTorch GitHub.
        #
        if language == 'chinese':
            tmp = cv2.resize(tmp, (128, 64))
        elif language == 'japanese':
            tmp = cv2.resize(tmp, (422, 64))
        elif language == 'english':
            tmp = cv2.resize(tmp, (680, 64))
        elif language == 'french':
            tmp = cv2.resize(tmp, (268, 64))
        elif language == 'korean':
            tmp = cv2.resize(tmp, (150, 64))
        elif language == 'thai':
            tmp = cv2.resize(tmp, (192, 64))
        else:
            exit()

        image = tmp[np.newaxis, np.newaxis, :, :]

        preds = net.predict(image)

        # Select max probabilty (greedy decoding) then decode index to character
        preds_size = [preds.shape[1]] * batch_size

        ######## filter ignore_char, rebalance
        preds_prob = softmax(preds, axis=2)
        preds_prob[:,:,ignore_idx] = 0.
        pred_norm = preds_prob.sum(axis=2)
        preds_prob = preds_prob/np.expand_dims(pred_norm, axis=-1)

        # Select max probabilty (greedy decoding) then decode index to character
        preds_index = np.argmax(preds_prob, axis=2)
        preds_index = np.squeeze(preds_index)
        preds_str = converter.decode_greedy(preds_index, preds_size)

        values = preds_prob.max(axis=2)
        indices = preds_prob.argmax(axis=2)
        preds_max_prob = []
        for v,i in zip(values, indices):
            max_probs = v[i!=0]
            if len(max_probs)>0:
                preds_max_prob.append(max_probs)
            else:
                preds_max_prob.append(np.array([0]))

        for pred, pred_max_prob in zip(preds_str, preds_max_prob):
            confidence_score = custom_mean(pred_max_prob)
            result.append([pred, confidence_score])

    return result


def get_text(language, character, imgH, imgW, recognizer, converter, image_list, ignore_char = ''):
    batch_max_length = int(imgW/10)

    ignore_idx = []
    for char in ignore_char:
        try: ignore_idx.append(character.index(char)+1)
        except: pass

    coord = [item[0] for item in image_list]
    img_list = [item[1] for item in image_list]
    input_list = [preprocess(img, imgH, imgW, keep_ratio_with_pad=True) for img in img_list]

    # predict first round
    result1 = recognize(language, recognizer, converter, input_list, batch_max_length, ignore_idx)

    # predict second round
    low_confident_idx = [i for i,item in enumerate(result1) if (item[1] < contrast_ths)]
    if len(low_confident_idx) > 0:
        img_list2 = [img_list[i] for i in low_confident_idx]
        input_list2 = [preprocess(img, imgH, imgW, keep_ratio_with_pad=True, adjust_contrast=adjust_contrast) for img in img_list2]

        result2 = recognize(language, recognizer, converter, input_list2, batch_max_length, ignore_idx)

    result = []
    for i, zipped in enumerate(zip(coord, result1)):
        box, pred1 = zipped
        if i in low_confident_idx:
            pred2 = result2[low_confident_idx.index(i)]
            if pred1[1]>pred2[1]:
                result.append( (box, pred1[0], pred1[1]) )
            else:
                result.append( (box, pred2[0], pred2[1]) )
        else:
            result.append( (box, pred1[0], pred1[1]) )
    return result


def recognizer_predict(language, lang_list, character, symbol, net, image_grey, horizontal_list, free_list):
    lang_char = set_language_list(lang_list, symbol)
    ignore_char = ''.join(set(character)-set(lang_char))
    separator_list = {}

    dict_list = {}
    for lang in lang_list:
        dict_list[lang] = os.path.join('config', lang + ".txt")

    converter = CTCLabelConverter(character, separator_list, dict_list)

    if (horizontal_list==None) and (free_list==None):
        y_max, x_max = image_grey.shape
        horizontal_list = [[0, x_max, 0, y_max]]
        free_list = []

    result = []
    for bbox in horizontal_list:
        h_list = [bbox]
        f_list = []
        image_list, max_width = get_image_list(h_list, f_list, image_grey)
        result0 = get_text(language, character, imgH, int(max_width), net, converter, image_list,\
                      ignore_char)
        result += result0
    for bbox in free_list:
        h_list = []
        f_list = [bbox]
        image_list, max_width = get_image_list(h_list, f_list, image_grey)
        result0 = get_text(language, character, imgH, int(max_width), net, converter, image_list,\
                      ignore_char)
        result += result0

    return result


def draw_ocr_box_txt(image, result, bbox_padding=0.):
    detected_image = image
    recognized_image = Image.new('RGB', (image.shape[1], image.shape[0]), (255, 255, 255))
    draw_recognized = ImageDraw.Draw(recognized_image)

    if sys.platform == "win32":
        # Windows
        font_path = 'C:/windows/Fonts/meiryo.ttc'
    elif sys.platform == "darwin":
        # Mac OS
        font_path = '/System/Library/Fonts/ヒラギノ丸ゴ ProN W4.ttc'
    else:
        # Linux
        font_path = '/usr/share/fonts/opentype/ipaexfont-gothic/ipaexg.ttf'

    for r in result:
        color = (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255))
        bbox = r[0]
        # detected image (left)
        cv2.rectangle(detected_image, (bbox[3][0], bbox[3][1]), (bbox[1][0], bbox[1][1]), color, thickness=2)
        # recognized image (right)
        draw_recognized.polygon([
            bbox[0][0], bbox[0][1], bbox[1][0], bbox[1][1], bbox[2][0],
            bbox[2][1], bbox[3][0], bbox[3][1]
        ], outline=color)
        box_height = math.sqrt((bbox[0][0] - bbox[3][0])**2 + (bbox[0][1] - bbox[3][1])**2)
        box_width = math.sqrt((bbox[0][0] - bbox[1][0])**2 + (bbox[0][1] - bbox[1][1])**2)
        if box_height > 2 * box_width:
            font_size = max(int(box_width * 0.9 * (1 - bbox_padding)), 10)
            font = ImageFont.truetype(font_path, font_size, encoding="utf-8")
            cur_y = bbox[0][1]
            for c in r[1]:
                char_size = font.getsize(c)
                draw_recognized.text(
                    (bbox[0][0] + 3, cur_y), c, fill=(0, 0, 0), font=font)
                cur_y += char_size[1]
        else:
            font_size = max(int(box_height * 0.8 * (1 - bbox_padding)), 10)
            font = ImageFont.truetype(font_path, font_size, encoding="utf-8")
            draw_recognized.text(
                [bbox[0][0], bbox[0][1]], r[1], fill=(0, 0, 0), font=font)

    recognized_image = np.array(recognized_image)
    image = cv2.hconcat([detected_image, recognized_image])

    return image
