#ailia predict api sample

import numpy as np
import time
import os
import sys
import cv2
from PIL import Image
from matplotlib import pyplot as plt
import argparse

import ailia

from lib_3d_bbox.Dataset import *
from lib_3d_bbox.Plotting import *
from lib_3d_bbox import ClassAverages
from lib_3d_bbox.Math import *

# import original modules
sys.path.append('../../util')
from utils import get_base_parser, update_parser, get_savepath  # noqa: E402
from model_utils import check_and_download_models  # noqa: E402
from detector_utils import plot_results, write_predictions, load_image  # noqa: E402
import webcamera_utils  # noqa: E402

# logger
from logging import getLogger   # noqa: E402
logger = getLogger(__name__)

os.environ['KMP_DUPLICATE_LIB_OK']='TRUE'

REMOTE_PATH_YOLOV3 = 'https://storage.googleapis.com/ailia-models/yolov3/'
REMOTE_PATH_3D_BBOX = 'https://storage.googleapis.com/ailia-models/3d_bbox/'

# settings
WEIGHT_PATH_YOLOV3 = "yolov3.opt.onnx"
MODEL_PATH_YOLOV3 = "yolov3.opt.onnx.prototxt"

WEIGHT_PATH_3D_BBOX = "3d_bbox.onnx"
MODEL_PATH_3D_BBOX = "3d_bbox.onnx.prototxt"

IMAGE_PATH = 'input.png'
SAVE_IMAGE_PATH = 'output.png'

CALIB_FILE = "./lib_3d_bbox/calib_cam_to_cam.txt"

COCO_CATEGORY = [
    "Pedestrian", "bicycle", "Car", "motorcycle", "airplane", "bus", "train",
    "Truck", "boat", "traffic light", "fire hydrant", "stop sign",
    "parking meter", "bench", "bird", "cat", "dog", "horse", "sheep", "cow",
    "elephant", "bear", "zebra", "giraffe", "backpack", "umbrella",
    "handbag", "tie", "suitcase", "frisbee", "skis", "snowboard",
    "sports ball", "kite", "baseball bat", "baseball glove", "skateboard",
    "surfboard", "tennis racket", "bottle", "wine glass", "cup", "fork",
    "knife", "spoon", "bowl", "banana", "apple", "sandwich", "orange",
    "broccoli", "carrot", "hot dog", "pizza", "donut", "cake", "chair",
    "couch", "potted plant", "bed", "dining table", "toilet", "tv",
    "laptop", "mouse", "remote", "keyboard", "cell phone", "microwave",
    "oven", "toaster", "sink", "refrigerator", "book", "clock", "vase",
    "scissors", "teddy bear", "hair drier", "toothbrush"
]
THRESHOLD = 0.4
IOU = 0.45

# Default input size
HEIGHT = 224
WIDTH = 224


def plot_regressed_3d_bbox(img, cam_to_img, box_2d, dimensions, alpha, theta_ray, img_2d=None):

    # the math! returns X, the corners used for constraint
    location, X = calc_location(dimensions, cam_to_img, box_2d, alpha, theta_ray)

    orient = alpha + theta_ray

    if img_2d is not None:
        plot_2d_box(img_2d, box_2d)

    plot_3d_box(img, cam_to_img, orient, dimensions, location) # 3d boxes

    return location

# ======================
# Arguemnt Parser Config
# ======================
parser = get_base_parser('3d_bbox model', IMAGE_PATH, SAVE_IMAGE_PATH)
args = update_parser(parser)

# ======================
# Main functions
# ======================
def recognize_from_image():
    averages = ClassAverages.ClassAverages()
    angle_bins = generate_bins(2)

    env_id = args.env_id
    net_yolov3 = ailia.Detector(
            MODEL_PATH_YOLOV3,
            WEIGHT_PATH_YOLOV3,
            len(COCO_CATEGORY),
            format=ailia.NETWORK_IMAGE_FORMAT_RGB,
            channel=ailia.NETWORK_IMAGE_CHANNEL_FIRST,
            range=ailia.NETWORK_IMAGE_RANGE_U_FP32,
            algorithm=ailia.DETECTOR_ALGORITHM_YOLOV3,
            env_id=env_id,
        )
    net_3d_bbox = ailia.Net(MODEL_PATH_3D_BBOX,WEIGHT_PATH_3D_BBOX,env_id=env_id)
    net_yolov3.set_input_shape(WIDTH, HEIGHT)

    # input image loop
    for image_path in args.input:
        # prepare input data
        logger.debug(f'input image: {image_path}')
        img = cv2.imread(image_path)
        logger.debug(f'input image shape: {img.shape}')
        yolo_img = cv2.cvtColor(img, cv2.COLOR_BGR2BGRA)
        logger.debug(f'yolo input image shape: {yolo_img.shape}')

        # inference
        logger.info('Start inference...')
        if args.benchmark:
            logger.info('BENCHMARK mode')
            for i in range(5):
                start = int(round(time.time() * 1000))
                detections = net_yolov3.run(yolo_img, THRESHOLD, IOU)
                end = int(round(time.time() * 1000))
                logger.info(f'\tailia processing time {end - start} ms')
        else:
            detections = net_yolov3.run(yolo_img, THRESHOLD, IOU)

        for detection in detections:
            detect_class = COCO_CATEGORY[detection[0]]
            xmin = int(detection[2][0] * img.shape[1])
            ymin = int(detection[2][1] * img.shape[0])
            xmax = int(detection[2][2] * img.shape[1]) + int(detection[2][0] * img.shape[1])
            ymax = int(detection[2][3] * img.shape[0]) + int(detection[2][1] * img.shape[0])
            detect_box_2d = [(xmin, ymin),(xmax, ymax)]
            if not averages.recognized_class(detect_class):
                continue
            try:
                detectedObject = DetectedObject(img, detect_class, detect_box_2d, CALIB_FILE)
            except:
                continue

            theta_ray = detectedObject.theta_ray
            input_img = detectedObject.img
            proj_matrix = detectedObject.proj_matrix
            box_2d = detect_box_2d
            detected_class = detect_class

            input_tensor = np.expand_dims(input_img,0)
            [orient, conf, dim] = net_3d_bbox.run(input_tensor)
            orient = orient[0, :, :]
            conf = conf[0, :]
            dim = dim[0, :]
            dim += averages.get_item(detected_class)

            argmax = np.argmax(conf)
            orient = orient[argmax, :]
            cos = orient[0]
            sin = orient[1]
            alpha = np.arctan2(sin, cos)
            alpha += angle_bins[argmax]
            alpha -= np.pi

            plot_regressed_3d_bbox(img, proj_matrix, box_2d, dim, alpha, theta_ray)
            cv2.imshow('3D detections', img)

        savepath = get_savepath(args.savepath, image_path)
        logger.info(f'saved at : {savepath}')
        cv2.imwrite(savepath, img)

    if cv2.waitKey(0) != 32:  # space bar
        exit()

def recognize_from_video():
    # net initialize
    averages = ClassAverages.ClassAverages()
    angle_bins = generate_bins(2)

    env_id = args.env_id
    net_yolov3 = ailia.Detector(
            MODEL_PATH_YOLOV3,
            WEIGHT_PATH_YOLOV3,
            len(COCO_CATEGORY),
            format=ailia.NETWORK_IMAGE_FORMAT_RGB,
            channel=ailia.NETWORK_IMAGE_CHANNEL_FIRST,
            range=ailia.NETWORK_IMAGE_RANGE_U_FP32,
            algorithm=ailia.DETECTOR_ALGORITHM_YOLOV3,
            env_id=env_id,
        )
    net_3d_bbox = ailia.Net(MODEL_PATH_3D_BBOX,WEIGHT_PATH_3D_BBOX,env_id=env_id)
    net_yolov3.set_input_shape(WIDTH, HEIGHT)

    capture = webcamera_utils.get_capture(args.video)

    # create video writer if savepath is specified as video format
    if args.savepath != SAVE_IMAGE_PATH:
        logger.warning(
            'currently, video results cannot be output correctly...'
        )
        f_h = int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT))
        f_w = int(capture.get(cv2.CAP_PROP_FRAME_WIDTH))
        save_h, save_w = f_h, f_w
        writer = webcamera_utils.get_writer(args.savepath, save_h, save_w)
    else:
        writer = None

    frame_shown = False
    while(True):
        ret, frame = capture.read()
        if (cv2.waitKey(1) & 0xFF == ord('q')) or not ret:
            break
        if frame_shown and cv2.getWindowProperty('3D detections', cv2.WND_PROP_VISIBLE) == 0:
            break

        img = frame
        yolo_img = cv2.cvtColor(img, cv2.COLOR_BGR2BGRA)

        detections = net_yolov3.run(yolo_img, THRESHOLD, IOU)
        for detection in detections:
            detect_class = COCO_CATEGORY[detection[0]]
            xmin = int(detection[2][0] * img.shape[1])
            ymin = int(detection[2][1] * img.shape[0])
            xmax = int(detection[2][2] * img.shape[1]) + int(detection[2][0] * img.shape[1])
            ymax = int(detection[2][3] * img.shape[0]) + int(detection[2][1] * img.shape[0])
            detect_box_2d = [(xmin, ymin), (xmax, ymax)]
            if not averages.recognized_class(detect_class):
                continue
            try:
                detectedObject = DetectedObject(img, detect_class, detect_box_2d, CALIB_FILE)
            except:
                continue

            theta_ray = detectedObject.theta_ray
            input_img = detectedObject.img
            proj_matrix = detectedObject.proj_matrix
            box_2d = detect_box_2d
            detected_class = detect_class

            input_tensor = np.expand_dims(input_img, 0)
            [orient, conf, dim] = net_3d_bbox.run(input_tensor)
            orient = orient[0, :, :]
            conf = conf[0, :]
            dim = dim[0, :]
            dim += averages.get_item(detected_class)

            argmax = np.argmax(conf)
            orient = orient[argmax, :]
            cos = orient[0]
            sin = orient[1]
            alpha = np.arctan2(sin, cos)
            alpha += angle_bins[argmax]
            alpha -= np.pi

            location = plot_regressed_3d_bbox(img, proj_matrix, box_2d, dim, alpha, theta_ray)

            cv2.imshow('3D detections', img)
            frame_shown = True

        # save results
        if writer is not None:
            writer.write(img)

    capture.release()
    cv2.destroyAllWindows()
    if writer is not None:
        writer.release()
    logger.info('Script finished successfully.')

def main():
    # model files check and download
    check_and_download_models(WEIGHT_PATH_3D_BBOX, MODEL_PATH_3D_BBOX, REMOTE_PATH_3D_BBOX)
    check_and_download_models(WEIGHT_PATH_YOLOV3, MODEL_PATH_YOLOV3, REMOTE_PATH_YOLOV3)

    if args.video is not None:
        # video mode
        recognize_from_video()
    else:
        # image mode
        recognize_from_image()


if __name__ == '__main__':
    main()
