
import sys
import time
from logging import getLogger

from funasr_onnx import SenseVoiceSmall
from funasr_onnx.utils.postprocess_utils import rich_transcription_postprocess
from funasr_onnx import Fsmn_vad_online

import soundfile
import librosa

import numpy as np

# import original modules
sys.path.append("../../util")

from model_utils import check_and_download_models, check_and_download_file  # noqa
from arg_utils import get_base_parser, get_savepath, update_parser  # noqa

logger = getLogger(__name__)

# ======================
# Parameters
# ======================

WAV_PATH = "ja.wav"
SAVE_TEXT_PATH = "output.txt"

# ======================
# Arguemnt Parser Config
# ======================

parser = get_base_parser("SenseVoice", WAV_PATH, SAVE_TEXT_PATH, input_ftype="audio", fp16_support = False)
#parser.add_argument(
#    "--intermediate", action="store_true", help="display intermediate state."
#)
#parser.add_argument(
#    "--fp16", action="store_true", help="use fp16 model (default : fp32 model)."
#)
parser.add_argument(
	"--onnx", action="store_true", help="use onnx runtime."
)
args = update_parser(parser)


# ======================
# Models
# ======================

WEIGHT_PATH = "sensevoice_small.onnx"
MODEL_PATH = "sensevoice_small.onnx.prototxt"

VAD_WEIGHT_PATH = "speech_fsmn_vad_zh-cn-16k-common.onnx"
VAD_MODEL_PATH = "speech_fsmn_vad_zh-cn-16k-common.onnx.prototxt"

REMOTE_PATH = "https://storage.googleapis.com/ailia-models/sensevoice/"

def recognize_from_audio():
	# input audio loop
	for audio_path in args.input:
		logger.info(audio_path)

		model = SenseVoiceSmall(env_id=args.env_id, onnx=args.onnx)
		vad = Fsmn_vad_online(env_id=args.env_id, onnx=args.onnx)

		# vad
		speech, sample_rate = soundfile.read(audio_path)
		if speech.ndim > 1:
			speech = np.mean(speech, axis=1)
		target_sr = 16000
		if sample_rate != target_sr:
			speech = librosa.resample(speech, orig_sr=sample_rate, target_sr=target_sr)

		vad_enable = True

		if vad_enable:
			speech_length = speech.shape[0]
			sample_offset = 0
			step = 1600
			param_dict = {"in_cache": []}
			start = -1
			end = -1

			start_profile = int(round(time.time() * 1000))

			for sample_offset in range(0, speech_length, min(step, speech_length - sample_offset)):
				if sample_offset + step >= speech_length - 1:
					step = speech_length - sample_offset
					is_final = True
				else:
					is_final = False
				param_dict["is_final"] = is_final
				segments_result = vad(
					audio_in=speech[sample_offset : sample_offset + step], param_dict=param_dict
				)
				for segment in segments_result:
					for s in segment:
						#print(s)
						if s[0] != -1:
							start = s[0]
						if s[1] != -1:
							end = s[1]
						if start != -1 and end != -1:
							start_int = int(start / 1000 * target_sr)
							end_int = int(end / 1000 * target_sr)
							audio = speech[start_int:end_int]

							res = model(audio, language="auto", use_itn=True)
							
							for i in res:
								print("[" + str(start / 1000) + " - " + str(end / 1000) + "] " + rich_transcription_postprocess(i))

							start = -1
							end = -1

			end_profile = int(round(time.time() * 1000))
			estimation_time = end_profile - start_profile
			logger.info(f"\ts2t processing time {estimation_time} ms")
		else:
			# s2t
			wav_or_scp = speech#[audio_path]
			#print(speech.shape)

			start = int(round(time.time() * 1000))
			res = model(wav_or_scp, language="auto", use_itn=True) # 16khz
			end = int(round(time.time() * 1000))
			estimation_time = end - start
			logger.info(f"\ts2t processing time {estimation_time} ms")
		
			print([rich_transcription_postprocess(i) for i in res])

		# inference
		logger.info("Start inference...")

	logger.info("Script finished successfully.")

def main():
	check_and_download_models(WEIGHT_PATH, MODEL_PATH, REMOTE_PATH)
	check_and_download_models(VAD_WEIGHT_PATH, VAD_MODEL_PATH, REMOTE_PATH)
	
	recognize_from_audio()

if __name__ == "__main__":
	main()
