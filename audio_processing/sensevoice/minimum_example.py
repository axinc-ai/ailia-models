# minimum infer example

import soundfile
import librosa

import numpy as np

import numpy as np

import librosa
import ailia.audio

from typing import List
from typing import Union

import librosa
import numpy as np
from typing import List, Union, Tuple, Dict

class SimpleDecoder():
	def fbank(self, waveform: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
		waveform = waveform * (1 << 15)
		waveform = np.asarray(waveform, dtype=np.float32)
		waveform += np.random.normal(scale=1.0, size=len(waveform)).astype(np.float32)
		mel_spec = ailia.audio.mel_spectrogram(
			wav=waveform,
			sample_rate=16000,
			fft_n=400,
			hop_n=160,
			win_n=400,
			win_type="hamming",
			mel_n=80,
			center_mode=False,
			power=2.0,
		)
		feat = np.log(np.maximum(mel_spec, 1e-10)).T.astype(np.float32)
		feat_len = np.array(feat.shape[0]).astype(np.int32)
		return feat, feat_len

	def lfr_cmvn(self, feat: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
		feat = self.apply_lfr(feat)
		feat = self.apply_cmvn(feat)
		feat_len = np.array(feat.shape[0]).astype(np.int32)
		return feat, feat_len

	def apply_lfr(self, inputs: np.ndarray) -> np.ndarray:
		LFR_inputs = []

		lfr_m = 7
		lfr_n = 6

		T = inputs.shape[0]
		T_lfr = int(np.ceil(T / lfr_n))
		left_padding = np.tile(inputs[0], ((lfr_m - 1) // 2, 1))
		inputs = np.vstack((left_padding, inputs))
		T = T + (lfr_m - 1) // 2
		for i in range(T_lfr):
			if lfr_m <= T - i * lfr_n:
				LFR_inputs.append((inputs[i * lfr_n : i * lfr_n + lfr_m]).reshape(1, -1))
			else:
				# process last LFR frame
				num_padding = lfr_m - (T - i * lfr_n)
				frame = inputs[i * lfr_n :].reshape(-1)
				for _ in range(num_padding):
					frame = np.hstack((frame, inputs[-1]))

				LFR_inputs.append(frame)
		LFR_outputs = np.vstack(LFR_inputs).astype(np.float32)
		return LFR_outputs

	def apply_cmvn(self, inputs: np.ndarray) -> np.ndarray:
		frame, dim = inputs.shape
		assert(dim == 560)
		means = np.array([-8.311879 ,-8.600912 ,-9.615928 ,-10.43595 ,-11.21292 ,-11.88333 ,-12.36243 ,-12.63706 ,-12.8818 ,-12.83066 ,-12.89103 ,-12.95666 ,-13.19763 ,-13.40598 ,-13.49113 ,-13.5546 ,-13.55639 ,-13.51915 ,-13.68284 ,-13.53289 ,-13.42107 ,-13.65519 ,-13.50713 ,-13.75251 ,-13.76715 ,-13.87408 ,-13.73109 ,-13.70412 ,-13.56073 ,-13.53488 ,-13.54895 ,-13.56228 ,-13.59408 ,-13.62047 ,-13.64198 ,-13.66109 ,-13.62669 ,-13.58297 ,-13.57387 ,-13.4739 ,-13.53063 ,-13.48348 ,-13.61047 ,-13.64716 ,-13.71546 ,-13.79184 ,-13.90614 ,-14.03098 ,-14.18205 ,-14.35881 ,-14.48419 ,-14.60172 ,-14.70591 ,-14.83362 ,-14.92122 ,-15.00622 ,-15.05122 ,-15.03119 ,-14.99028 ,-14.92302 ,-14.86927 ,-14.82691 ,-14.7972 ,-14.76909 ,-14.71356 ,-14.61277 ,-14.51696 ,-14.42252 ,-14.36405 ,-14.30451 ,-14.23161 ,-14.19851 ,-14.16633 ,-14.15649 ,-14.10504 ,-13.99518 ,-13.79562 ,-13.3996 ,-12.7767 ,-11.71208 ,-8.311879 ,-8.600912 ,-9.615928 ,-10.43595 ,-11.21292 ,-11.88333 ,-12.36243 ,-12.63706 ,-12.8818 ,-12.83066 ,-12.89103 ,-12.95666 ,-13.19763 ,-13.40598 ,-13.49113 ,-13.5546 ,-13.55639 ,-13.51915 ,-13.68284 ,-13.53289 ,-13.42107 ,-13.65519 ,-13.50713 ,-13.75251 ,-13.76715 ,-13.87408 ,-13.73109 ,-13.70412 ,-13.56073 ,-13.53488 ,-13.54895 ,-13.56228 ,-13.59408 ,-13.62047 ,-13.64198 ,-13.66109 ,-13.62669 ,-13.58297 ,-13.57387 ,-13.4739 ,-13.53063 ,-13.48348 ,-13.61047 ,-13.64716 ,-13.71546 ,-13.79184 ,-13.90614 ,-14.03098 ,-14.18205 ,-14.35881 ,-14.48419 ,-14.60172 ,-14.70591 ,-14.83362 ,-14.92122 ,-15.00622 ,-15.05122 ,-15.03119 ,-14.99028 ,-14.92302 ,-14.86927 ,-14.82691 ,-14.7972 ,-14.76909 ,-14.71356 ,-14.61277 ,-14.51696 ,-14.42252 ,-14.36405 ,-14.30451 ,-14.23161 ,-14.19851 ,-14.16633 ,-14.15649 ,-14.10504 ,-13.99518 ,-13.79562 ,-13.3996 ,-12.7767 ,-11.71208 ,-8.311879 ,-8.600912 ,-9.615928 ,-10.43595 ,-11.21292 ,-11.88333 ,-12.36243 ,-12.63706 ,-12.8818 ,-12.83066 ,-12.89103 ,-12.95666 ,-13.19763 ,-13.40598 ,-13.49113 ,-13.5546 ,-13.55639 ,-13.51915 ,-13.68284 ,-13.53289 ,-13.42107 ,-13.65519 ,-13.50713 ,-13.75251 ,-13.76715 ,-13.87408 ,-13.73109 ,-13.70412 ,-13.56073 ,-13.53488 ,-13.54895 ,-13.56228 ,-13.59408 ,-13.62047 ,-13.64198 ,-13.66109 ,-13.62669 ,-13.58297 ,-13.57387 ,-13.4739 ,-13.53063 ,-13.48348 ,-13.61047 ,-13.64716 ,-13.71546 ,-13.79184 ,-13.90614 ,-14.03098 ,-14.18205 ,-14.35881 ,-14.48419 ,-14.60172 ,-14.70591 ,-14.83362 ,-14.92122 ,-15.00622 ,-15.05122 ,-15.03119 ,-14.99028 ,-14.92302 ,-14.86927 ,-14.82691 ,-14.7972 ,-14.76909 ,-14.71356 ,-14.61277 ,-14.51696 ,-14.42252 ,-14.36405 ,-14.30451 ,-14.23161 ,-14.19851 ,-14.16633 ,-14.15649 ,-14.10504 ,-13.99518 ,-13.79562 ,-13.3996 ,-12.7767 ,-11.71208 ,-8.311879 ,-8.600912 ,-9.615928 ,-10.43595 ,-11.21292 ,-11.88333 ,-12.36243 ,-12.63706 ,-12.8818 ,-12.83066 ,-12.89103 ,-12.95666 ,-13.19763 ,-13.40598 ,-13.49113 ,-13.5546 ,-13.55639 ,-13.51915 ,-13.68284 ,-13.53289 ,-13.42107 ,-13.65519 ,-13.50713 ,-13.75251 ,-13.76715 ,-13.87408 ,-13.73109 ,-13.70412 ,-13.56073 ,-13.53488 ,-13.54895 ,-13.56228 ,-13.59408 ,-13.62047 ,-13.64198 ,-13.66109 ,-13.62669 ,-13.58297 ,-13.57387 ,-13.4739 ,-13.53063 ,-13.48348 ,-13.61047 ,-13.64716 ,-13.71546 ,-13.79184 ,-13.90614 ,-14.03098 ,-14.18205 ,-14.35881 ,-14.48419 ,-14.60172 ,-14.70591 ,-14.83362 ,-14.92122 ,-15.00622 ,-15.05122 ,-15.03119 ,-14.99028 ,-14.92302 ,-14.86927 ,-14.82691 ,-14.7972 ,-14.76909 ,-14.71356 ,-14.61277 ,-14.51696 ,-14.42252 ,-14.36405 ,-14.30451 ,-14.23161 ,-14.19851 ,-14.16633 ,-14.15649 ,-14.10504 ,-13.99518 ,-13.79562 ,-13.3996 ,-12.7767 ,-11.71208 ,-8.311879 ,-8.600912 ,-9.615928 ,-10.43595 ,-11.21292 ,-11.88333 ,-12.36243 ,-12.63706 ,-12.8818 ,-12.83066 ,-12.89103 ,-12.95666 ,-13.19763 ,-13.40598 ,-13.49113 ,-13.5546 ,-13.55639 ,-13.51915 ,-13.68284 ,-13.53289 ,-13.42107 ,-13.65519 ,-13.50713 ,-13.75251 ,-13.76715 ,-13.87408 ,-13.73109 ,-13.70412 ,-13.56073 ,-13.53488 ,-13.54895 ,-13.56228 ,-13.59408 ,-13.62047 ,-13.64198 ,-13.66109 ,-13.62669 ,-13.58297 ,-13.57387 ,-13.4739 ,-13.53063 ,-13.48348 ,-13.61047 ,-13.64716 ,-13.71546 ,-13.79184 ,-13.90614 ,-14.03098 ,-14.18205 ,-14.35881 ,-14.48419 ,-14.60172 ,-14.70591 ,-14.83362 ,-14.92122 ,-15.00622 ,-15.05122 ,-15.03119 ,-14.99028 ,-14.92302 ,-14.86927 ,-14.82691 ,-14.7972 ,-14.76909 ,-14.71356 ,-14.61277 ,-14.51696 ,-14.42252 ,-14.36405 ,-14.30451 ,-14.23161 ,-14.19851 ,-14.16633 ,-14.15649 ,-14.10504 ,-13.99518 ,-13.79562 ,-13.3996 ,-12.7767 ,-11.71208 ,-8.311879 ,-8.600912 ,-9.615928 ,-10.43595 ,-11.21292 ,-11.88333 ,-12.36243 ,-12.63706 ,-12.8818 ,-12.83066 ,-12.89103 ,-12.95666 ,-13.19763 ,-13.40598 ,-13.49113 ,-13.5546 ,-13.55639 ,-13.51915 ,-13.68284 ,-13.53289 ,-13.42107 ,-13.65519 ,-13.50713 ,-13.75251 ,-13.76715 ,-13.87408 ,-13.73109 ,-13.70412 ,-13.56073 ,-13.53488 ,-13.54895 ,-13.56228 ,-13.59408 ,-13.62047 ,-13.64198 ,-13.66109 ,-13.62669 ,-13.58297 ,-13.57387 ,-13.4739 ,-13.53063 ,-13.48348 ,-13.61047 ,-13.64716 ,-13.71546 ,-13.79184 ,-13.90614 ,-14.03098 ,-14.18205 ,-14.35881 ,-14.48419 ,-14.60172 ,-14.70591 ,-14.83362 ,-14.92122 ,-15.00622 ,-15.05122 ,-15.03119 ,-14.99028 ,-14.92302 ,-14.86927 ,-14.82691 ,-14.7972 ,-14.76909 ,-14.71356 ,-14.61277 ,-14.51696 ,-14.42252 ,-14.36405 ,-14.30451 ,-14.23161 ,-14.19851 ,-14.16633 ,-14.15649 ,-14.10504 ,-13.99518 ,-13.79562 ,-13.3996 ,-12.7767 ,-11.71208 ,-8.311879 ,-8.600912 ,-9.615928 ,-10.43595 ,-11.21292 ,-11.88333 ,-12.36243 ,-12.63706 ,-12.8818 ,-12.83066 ,-12.89103 ,-12.95666 ,-13.19763 ,-13.40598 ,-13.49113 ,-13.5546 ,-13.55639 ,-13.51915 ,-13.68284 ,-13.53289 ,-13.42107 ,-13.65519 ,-13.50713 ,-13.75251 ,-13.76715 ,-13.87408 ,-13.73109 ,-13.70412 ,-13.56073 ,-13.53488 ,-13.54895 ,-13.56228 ,-13.59408 ,-13.62047 ,-13.64198 ,-13.66109 ,-13.62669 ,-13.58297 ,-13.57387 ,-13.4739 ,-13.53063 ,-13.48348 ,-13.61047 ,-13.64716 ,-13.71546 ,-13.79184 ,-13.90614 ,-14.03098 ,-14.18205 ,-14.35881 ,-14.48419 ,-14.60172 ,-14.70591 ,-14.83362 ,-14.92122 ,-15.00622 ,-15.05122 ,-15.03119 ,-14.99028 ,-14.92302 ,-14.86927 ,-14.82691 ,-14.7972 ,-14.76909 ,-14.71356 ,-14.61277 ,-14.51696 ,-14.42252 ,-14.36405 ,-14.30451 ,-14.23161 ,-14.19851 ,-14.16633 ,-14.15649 ,-14.10504 ,-13.99518 ,-13.79562 ,-13.3996 ,-12.7767 ,-11.71208], dtype=np.float64)
		vars = np.array([0.155775 ,0.154484 ,0.1527379 ,0.1518718 ,0.1506028 ,0.1489256 ,0.147067 ,0.1447061 ,0.1436307 ,0.1443568 ,0.1451849 ,0.1455157 ,0.1452821 ,0.1445717 ,0.1439195 ,0.1435867 ,0.1436018 ,0.1438781 ,0.1442086 ,0.1448844 ,0.1454756 ,0.145663 ,0.146268 ,0.1467386 ,0.1472724 ,0.147664 ,0.1480913 ,0.1483739 ,0.1488841 ,0.1493636 ,0.1497088 ,0.1500379 ,0.1502916 ,0.1505389 ,0.1506787 ,0.1507102 ,0.1505992 ,0.1505445 ,0.1505938 ,0.1508133 ,0.1509569 ,0.1512396 ,0.1514625 ,0.1516195 ,0.1516156 ,0.1515561 ,0.1514966 ,0.1513976 ,0.1512612 ,0.151076 ,0.1510596 ,0.1510431 ,0.151077 ,0.1511168 ,0.1511917 ,0.151023 ,0.1508045 ,0.1505885 ,0.1503493 ,0.1502373 ,0.1501726 ,0.1500762 ,0.1500065 ,0.1499782 ,0.150057 ,0.1502658 ,0.150469 ,0.1505335 ,0.1505505 ,0.1505328 ,0.1504275 ,0.1502438 ,0.1499674 ,0.1497118 ,0.1494661 ,0.1493102 ,0.1493681 ,0.1495501 ,0.1499738 ,0.1509654 ,0.155775 ,0.154484 ,0.1527379 ,0.1518718 ,0.1506028 ,0.1489256 ,0.147067 ,0.1447061 ,0.1436307 ,0.1443568 ,0.1451849 ,0.1455157 ,0.1452821 ,0.1445717 ,0.1439195 ,0.1435867 ,0.1436018 ,0.1438781 ,0.1442086 ,0.1448844 ,0.1454756 ,0.145663 ,0.146268 ,0.1467386 ,0.1472724 ,0.147664 ,0.1480913 ,0.1483739 ,0.1488841 ,0.1493636 ,0.1497088 ,0.1500379 ,0.1502916 ,0.1505389 ,0.1506787 ,0.1507102 ,0.1505992 ,0.1505445 ,0.1505938 ,0.1508133 ,0.1509569 ,0.1512396 ,0.1514625 ,0.1516195 ,0.1516156 ,0.1515561 ,0.1514966 ,0.1513976 ,0.1512612 ,0.151076 ,0.1510596 ,0.1510431 ,0.151077 ,0.1511168 ,0.1511917 ,0.151023 ,0.1508045 ,0.1505885 ,0.1503493 ,0.1502373 ,0.1501726 ,0.1500762 ,0.1500065 ,0.1499782 ,0.150057 ,0.1502658 ,0.150469 ,0.1505335 ,0.1505505 ,0.1505328 ,0.1504275 ,0.1502438 ,0.1499674 ,0.1497118 ,0.1494661 ,0.1493102 ,0.1493681 ,0.1495501 ,0.1499738 ,0.1509654 ,0.155775 ,0.154484 ,0.1527379 ,0.1518718 ,0.1506028 ,0.1489256 ,0.147067 ,0.1447061 ,0.1436307 ,0.1443568 ,0.1451849 ,0.1455157 ,0.1452821 ,0.1445717 ,0.1439195 ,0.1435867 ,0.1436018 ,0.1438781 ,0.1442086 ,0.1448844 ,0.1454756 ,0.145663 ,0.146268 ,0.1467386 ,0.1472724 ,0.147664 ,0.1480913 ,0.1483739 ,0.1488841 ,0.1493636 ,0.1497088 ,0.1500379 ,0.1502916 ,0.1505389 ,0.1506787 ,0.1507102 ,0.1505992 ,0.1505445 ,0.1505938 ,0.1508133 ,0.1509569 ,0.1512396 ,0.1514625 ,0.1516195 ,0.1516156 ,0.1515561 ,0.1514966 ,0.1513976 ,0.1512612 ,0.151076 ,0.1510596 ,0.1510431 ,0.151077 ,0.1511168 ,0.1511917 ,0.151023 ,0.1508045 ,0.1505885 ,0.1503493 ,0.1502373 ,0.1501726 ,0.1500762 ,0.1500065 ,0.1499782 ,0.150057 ,0.1502658 ,0.150469 ,0.1505335 ,0.1505505 ,0.1505328 ,0.1504275 ,0.1502438 ,0.1499674 ,0.1497118 ,0.1494661 ,0.1493102 ,0.1493681 ,0.1495501 ,0.1499738 ,0.1509654 ,0.155775 ,0.154484 ,0.1527379 ,0.1518718 ,0.1506028 ,0.1489256 ,0.147067 ,0.1447061 ,0.1436307 ,0.1443568 ,0.1451849 ,0.1455157 ,0.1452821 ,0.1445717 ,0.1439195 ,0.1435867 ,0.1436018 ,0.1438781 ,0.1442086 ,0.1448844 ,0.1454756 ,0.145663 ,0.146268 ,0.1467386 ,0.1472724 ,0.147664 ,0.1480913 ,0.1483739 ,0.1488841 ,0.1493636 ,0.1497088 ,0.1500379 ,0.1502916 ,0.1505389 ,0.1506787 ,0.1507102 ,0.1505992 ,0.1505445 ,0.1505938 ,0.1508133 ,0.1509569 ,0.1512396 ,0.1514625 ,0.1516195 ,0.1516156 ,0.1515561 ,0.1514966 ,0.1513976 ,0.1512612 ,0.151076 ,0.1510596 ,0.1510431 ,0.151077 ,0.1511168 ,0.1511917 ,0.151023 ,0.1508045 ,0.1505885 ,0.1503493 ,0.1502373 ,0.1501726 ,0.1500762 ,0.1500065 ,0.1499782 ,0.150057 ,0.1502658 ,0.150469 ,0.1505335 ,0.1505505 ,0.1505328 ,0.1504275 ,0.1502438 ,0.1499674 ,0.1497118 ,0.1494661 ,0.1493102 ,0.1493681 ,0.1495501 ,0.1499738 ,0.1509654 ,0.155775 ,0.154484 ,0.1527379 ,0.1518718 ,0.1506028 ,0.1489256 ,0.147067 ,0.1447061 ,0.1436307 ,0.1443568 ,0.1451849 ,0.1455157 ,0.1452821 ,0.1445717 ,0.1439195 ,0.1435867 ,0.1436018 ,0.1438781 ,0.1442086 ,0.1448844 ,0.1454756 ,0.145663 ,0.146268 ,0.1467386 ,0.1472724 ,0.147664 ,0.1480913 ,0.1483739 ,0.1488841 ,0.1493636 ,0.1497088 ,0.1500379 ,0.1502916 ,0.1505389 ,0.1506787 ,0.1507102 ,0.1505992 ,0.1505445 ,0.1505938 ,0.1508133 ,0.1509569 ,0.1512396 ,0.1514625 ,0.1516195 ,0.1516156 ,0.1515561 ,0.1514966 ,0.1513976 ,0.1512612 ,0.151076 ,0.1510596 ,0.1510431 ,0.151077 ,0.1511168 ,0.1511917 ,0.151023 ,0.1508045 ,0.1505885 ,0.1503493 ,0.1502373 ,0.1501726 ,0.1500762 ,0.1500065 ,0.1499782 ,0.150057 ,0.1502658 ,0.150469 ,0.1505335 ,0.1505505 ,0.1505328 ,0.1504275 ,0.1502438 ,0.1499674 ,0.1497118 ,0.1494661 ,0.1493102 ,0.1493681 ,0.1495501 ,0.1499738 ,0.1509654 ,0.155775 ,0.154484 ,0.1527379 ,0.1518718 ,0.1506028 ,0.1489256 ,0.147067 ,0.1447061 ,0.1436307 ,0.1443568 ,0.1451849 ,0.1455157 ,0.1452821 ,0.1445717 ,0.1439195 ,0.1435867 ,0.1436018 ,0.1438781 ,0.1442086 ,0.1448844 ,0.1454756 ,0.145663 ,0.146268 ,0.1467386 ,0.1472724 ,0.147664 ,0.1480913 ,0.1483739 ,0.1488841 ,0.1493636 ,0.1497088 ,0.1500379 ,0.1502916 ,0.1505389 ,0.1506787 ,0.1507102 ,0.1505992 ,0.1505445 ,0.1505938 ,0.1508133 ,0.1509569 ,0.1512396 ,0.1514625 ,0.1516195 ,0.1516156 ,0.1515561 ,0.1514966 ,0.1513976 ,0.1512612 ,0.151076 ,0.1510596 ,0.1510431 ,0.151077 ,0.1511168 ,0.1511917 ,0.151023 ,0.1508045 ,0.1505885 ,0.1503493 ,0.1502373 ,0.1501726 ,0.1500762 ,0.1500065 ,0.1499782 ,0.150057 ,0.1502658 ,0.150469 ,0.1505335 ,0.1505505 ,0.1505328 ,0.1504275 ,0.1502438 ,0.1499674 ,0.1497118 ,0.1494661 ,0.1493102 ,0.1493681 ,0.1495501 ,0.1499738 ,0.1509654 ,0.155775 ,0.154484 ,0.1527379 ,0.1518718 ,0.1506028 ,0.1489256 ,0.147067 ,0.1447061 ,0.1436307 ,0.1443568 ,0.1451849 ,0.1455157 ,0.1452821 ,0.1445717 ,0.1439195 ,0.1435867 ,0.1436018 ,0.1438781 ,0.1442086 ,0.1448844 ,0.1454756 ,0.145663 ,0.146268 ,0.1467386 ,0.1472724 ,0.147664 ,0.1480913 ,0.1483739 ,0.1488841 ,0.1493636 ,0.1497088 ,0.1500379 ,0.1502916 ,0.1505389 ,0.1506787 ,0.1507102 ,0.1505992 ,0.1505445 ,0.1505938 ,0.1508133 ,0.1509569 ,0.1512396 ,0.1514625 ,0.1516195 ,0.1516156 ,0.1515561 ,0.1514966 ,0.1513976 ,0.1512612 ,0.151076 ,0.1510596 ,0.1510431 ,0.151077 ,0.1511168 ,0.1511917 ,0.151023 ,0.1508045 ,0.1505885 ,0.1503493 ,0.1502373 ,0.1501726 ,0.1500762 ,0.1500065 ,0.1499782 ,0.150057 ,0.1502658 ,0.150469 ,0.1505335 ,0.1505505 ,0.1505328 ,0.1504275 ,0.1502438 ,0.1499674 ,0.1497118 ,0.1494661 ,0.1493102 ,0.1493681 ,0.1495501 ,0.1499738 ,0.1509654], dtype=np.float64)
		means = np.tile(means, (frame, 1))
		vars = np.tile(vars, (frame, 1))
		inputs = (inputs + means) * vars
		return inputs

	def _get_lid(self, lid):
		if lid in list(self.lid_dict.keys()):
			return self.lid_dict[lid]
		else:
			raise ValueError(
				f"The language {lid} is not in {list(self.lid_dict.keys())}"
			)
			
	def _get_tnid(self, tnid):
		if tnid in list(self.textnorm_dict.keys()):
			return self.textnorm_dict[tnid]
		else:
			raise ValueError(
				f"The textnorm {tnid} is not in {list(self.textnorm_dict.keys())}"
			)
	
	def read_tags(self, language_input, textnorm_input):
		language_list = [self._get_lid(language_input)]
		textnorm_list = [self._get_tnid(textnorm_input)]
		return language_list, textnorm_list

	def __call__(self, wav_content: Union[str, np.ndarray, List[str]], **kwargs):
		import ailia
		self.model = ailia.Net(weight="sensevoice_small.onnx", env_id=1, memory_mode=11)

		from ailia_tokenizer import LlamaTokenizer
		self.tokenizer = LlamaTokenizer.from_pretrained("./tokenizer")

		self.batch_size = 1
		self.blank_id = 0
		self.lid_dict = {"auto": 0, "zh": 3, "en": 4, "yue": 7, "ja": 11, "ko": 12, "nospeech": 13}
		self.lid_int_dict = {24884: 3, 24885: 4, 24888: 7, 24892: 11, 24896: 12, 24992: 13}
		self.textnorm_dict = {"withitn": 14, "woitn": 15} # ãƒ†ã‚­ã‚¹ãƒˆã®æ­£è¦åŒ–ã‚’è¡Œã†ã‹ã©ã†ã‹
		self.textnorm_int_dict = {25016: 14, 25017: 15}

		language_input = kwargs.get("language", "auto")
		textnorm_input = kwargs.get("textnorm", "woitn")
		language_list, textnorm_list = self.read_tags(language_input, textnorm_input)
		
		asr_res = []

		feats, feats_len = self.extract_feat(wav_content)
		_language_list = language_list
		_textnorm_list = textnorm_list
		ctc_logits, encoder_out_lens = self.model.run([
			feats,
			feats_len,
			np.array(_language_list, dtype=np.int32),
			np.array(_textnorm_list, dtype=np.int32),
		])

		x = ctc_logits[0, : encoder_out_lens[0].item(), :]
		yseq = np.argmax(x, axis=-1)
		# Use np.diff and np.where instead of torch.unique_consecutive.
		mask = np.concatenate(([True], np.diff(yseq) != 0))
		yseq = yseq[mask]

		mask = yseq != self.blank_id
		token_int = yseq[mask].tolist()

		text = self.tokenizer.decode(token_int, skip_special_tokens=True)
		asr_res.append(text)

		return asr_res

	def extract_feat(self, waveform) -> Tuple[np.ndarray, np.ndarray]:
		speech, _ = self.fbank(waveform)
		feat, feat_len = self.lfr_cmvn(speech)
		return np.expand_dims(feat, axis=0), np.expand_dims(feat_len, axis=0)

emo_dict = {
    "<|HAPPY|>": "ğŸ˜Š",
    "<|SAD|>": "ğŸ˜”",
    "<|ANGRY|>": "ğŸ˜¡",
    "<|NEUTRAL|>": "",
    "<|FEARFUL|>": "ğŸ˜°",
    "<|DISGUSTED|>": "ğŸ¤¢",
    "<|SURPRISED|>": "ğŸ˜®",
}

event_dict = {
    "<|BGM|>": "ğŸ¼",
    "<|Speech|>": "",
    "<|Applause|>": "ğŸ‘",
    "<|Laughter|>": "ğŸ˜€",
    "<|Cry|>": "ğŸ˜­",
    "<|Sneeze|>": "ğŸ¤§",
    "<|Breath|>": "",
    "<|Cough|>": "ğŸ¤§",
}

lang_dict = {
    "<|zh|>": "<|lang|>",
    "<|en|>": "<|lang|>",
    "<|yue|>": "<|lang|>",
    "<|ja|>": "<|lang|>",
    "<|ko|>": "<|lang|>",
    "<|nospeech|>": "<|lang|>",
}

emoji_dict = {
    "<|nospeech|><|Event_UNK|>": "â“",
    "<|zh|>": "",
    "<|en|>": "",
    "<|yue|>": "",
    "<|ja|>": "",
    "<|ko|>": "",
    "<|nospeech|>": "",
    "<|HAPPY|>": "ğŸ˜Š",
    "<|SAD|>": "ğŸ˜”",
    "<|ANGRY|>": "ğŸ˜¡",
    "<|NEUTRAL|>": "",
    "<|BGM|>": "ğŸ¼",
    "<|Speech|>": "",
    "<|Applause|>": "ğŸ‘",
    "<|Laughter|>": "ğŸ˜€",
    "<|FEARFUL|>": "ğŸ˜°",
    "<|DISGUSTED|>": "ğŸ¤¢",
    "<|SURPRISED|>": "ğŸ˜®",
    "<|Cry|>": "ğŸ˜­",
    "<|EMO_UNKNOWN|>": "",
    "<|Sneeze|>": "ğŸ¤§",
    "<|Breath|>": "",
    "<|Cough|>": "ğŸ˜·",
    "<|Sing|>": "",
    "<|Speech_Noise|>": "",
    "<|withitn|>": "",
    "<|woitn|>": "",
    "<|GBG|>": "",
    "<|Event_UNK|>": "",
}

emo_set = {"ğŸ˜Š", "ğŸ˜”", "ğŸ˜¡", "ğŸ˜°", "ğŸ¤¢", "ğŸ˜®"}
event_set = {
    "ğŸ¼",
    "ğŸ‘",
    "ğŸ˜€",
    "ğŸ˜­",
    "ğŸ¤§",
    "ğŸ˜·",
}


def format_str_v2(s):
    sptk_dict = {}
    for sptk in emoji_dict:
        sptk_dict[sptk] = s.count(sptk)
        s = s.replace(sptk, "")
    emo = "<|NEUTRAL|>"
    for e in emo_dict:
        if sptk_dict[e] > sptk_dict[emo]:
            emo = e
    for e in event_dict:
        if sptk_dict[e] > 0:
            s = event_dict[e] + s
    s = s + emo_dict[emo]

    for emoji in emo_set.union(event_set):
        s = s.replace(" " + emoji, emoji)
        s = s.replace(emoji + " ", emoji)
    return s.strip()


def rich_transcription_postprocess(s):
    def get_emo(s):
        return s[-1] if s[-1] in emo_set else None

    def get_event(s):
        return s[0] if s[0] in event_set else None

    s = s.replace("<|nospeech|><|Event_UNK|>", "â“")
    for lang in lang_dict:
        s = s.replace(lang, "<|lang|>")
    s_list = [format_str_v2(s_i).strip(" ") for s_i in s.split("<|lang|>")]
    new_s = " " + s_list[0]
    cur_ent_event = get_event(new_s)
    for i in range(1, len(s_list)):
        if len(s_list[i]) == 0:
            continue
        if get_event(s_list[i]) == cur_ent_event and get_event(s_list[i]) != None:
            s_list[i] = s_list[i][1:]
        # else:
        cur_ent_event = get_event(s_list[i])
        if get_emo(s_list[i]) != None and get_emo(s_list[i]) == get_emo(new_s):
            new_s = new_s[:-1]
        new_s += s_list[i].strip().lstrip()
    new_s = new_s.replace("The.", " ")
    return new_s.strip()

def main():
	path = "ja.wav"
	#path = "ax.wav"
	speech, sample_rate = soundfile.read(path)
	if speech.ndim > 1:
		speech = np.mean(speech, axis=1)
	target_sr = 16000
	if sample_rate != target_sr:
		speech = librosa.resample(speech, orig_sr=sample_rate, target_sr=target_sr)

	wav_or_scp = speech
	decoder = SimpleDecoder()
	res = decoder(wav_or_scp, language="auto", use_itn=True) # 16khz
	print(rich_transcription_postprocess(res[0]))

if __name__ == "__main__":
	main()
