# minimum infer example

import soundfile
import librosa

import numpy as np

import numpy as np

import librosa
import ailia.audio

from typing import List
from typing import Union

import os.path
import librosa
import numpy as np
from typing import List, Union, Tuple, Dict

frame_opts_samp_freq = 16000
mel_opts_num_bins = 80
frame_opts_window_type = "hamming"
frame_opts_frame_length_ms = 25
frame_opts_frame_shift_ms = 10

class SimpleDecoder():
	def accept_waveform(self, sampling_rate: float, waveform):
		self.sr = int(frame_opts_samp_freq)
		self.dither = 1.0
		self.window_type = frame_opts_window_type
		self.frame_length_ms = float(frame_opts_frame_length_ms)
		self.frame_shift_ms = float(frame_opts_frame_shift_ms)
		self.n_mels = int(mel_opts_num_bins)
		self.frame_length = int(self.sr * self.frame_length_ms / 1000)
		self.frame_shift = int(self.sr * self.frame_shift_ms / 1000)
		self.snip_edges = True
		self._waveform_buffer = np.array([], dtype=np.float32)
		self._frames_cache = None
		self.preemphasis_coefficient = 0.97
			
		waveform = np.asarray(waveform, dtype=np.float32)
		waveform += np.random.normal(scale=self.dither, size=len(waveform)).astype(np.float32)
		self._waveform_buffer = np.concatenate([self._waveform_buffer, waveform])

		hop_length = self.frame_shift
		win_length = self.frame_length

		mel_spec = ailia.audio.mel_spectrogram(
			wav=self._waveform_buffer,
			sample_rate=self.sr,
			fft_n= win_length,
			hop_n=hop_length,
			win_n=win_length,
			win_type=self.window_type,
			mel_n=self.n_mels,
			center_mode=not self.snip_edges,
			power=2.0,
		)
		mel_spec = np.log(np.maximum(mel_spec, 1e-10)).T.astype(np.float32)
		self._frames_cache = mel_spec

	def fbank(self, waveform: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
		waveform = waveform * (1 << 15)
		self.accept_waveform(frame_opts_samp_freq, waveform.tolist())
		frames = self._frames_cache.shape[0]
		mat = np.empty([frames, mel_opts_num_bins])
		for i in range(frames):
			mat[i, :] = self._frames_cache[i]
		feat = mat.astype(np.float32)
		feat_len = np.array(mat.shape[0]).astype(np.int32)
		return feat, feat_len

	def lfr_cmvn(self, feat: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
		self.lfr_m = 7
		self.lfr_n = 6

		feat = self.apply_lfr(feat, self.lfr_m, self.lfr_n)
		feat = self.apply_cmvn(feat)

		feat_len = np.array(feat.shape[0]).astype(np.int32)
		return feat, feat_len

	@staticmethod
	def apply_lfr(inputs: np.ndarray, lfr_m: int, lfr_n: int) -> np.ndarray:
		LFR_inputs = []

		T = inputs.shape[0]
		T_lfr = int(np.ceil(T / lfr_n))
		left_padding = np.tile(inputs[0], ((lfr_m - 1) // 2, 1))
		inputs = np.vstack((left_padding, inputs))
		T = T + (lfr_m - 1) // 2
		for i in range(T_lfr):
			if lfr_m <= T - i * lfr_n:
				LFR_inputs.append((inputs[i * lfr_n : i * lfr_n + lfr_m]).reshape(1, -1))
			else:
				# process last LFR frame
				num_padding = lfr_m - (T - i * lfr_n)
				frame = inputs[i * lfr_n :].reshape(-1)
				for _ in range(num_padding):
					frame = np.hstack((frame, inputs[-1]))

				LFR_inputs.append(frame)
		LFR_outputs = np.vstack(LFR_inputs).astype(np.float32)
		return LFR_outputs

	def apply_cmvn(self, inputs: np.ndarray) -> np.ndarray:
		frame, dim = inputs.shape
		assert(dim == 560)
		means = np.array([-8.311879 ,-8.600912 ,-9.615928 ,-10.43595 ,-11.21292 ,-11.88333 ,-12.36243 ,-12.63706 ,-12.8818 ,-12.83066 ,-12.89103 ,-12.95666 ,-13.19763 ,-13.40598 ,-13.49113 ,-13.5546 ,-13.55639 ,-13.51915 ,-13.68284 ,-13.53289 ,-13.42107 ,-13.65519 ,-13.50713 ,-13.75251 ,-13.76715 ,-13.87408 ,-13.73109 ,-13.70412 ,-13.56073 ,-13.53488 ,-13.54895 ,-13.56228 ,-13.59408 ,-13.62047 ,-13.64198 ,-13.66109 ,-13.62669 ,-13.58297 ,-13.57387 ,-13.4739 ,-13.53063 ,-13.48348 ,-13.61047 ,-13.64716 ,-13.71546 ,-13.79184 ,-13.90614 ,-14.03098 ,-14.18205 ,-14.35881 ,-14.48419 ,-14.60172 ,-14.70591 ,-14.83362 ,-14.92122 ,-15.00622 ,-15.05122 ,-15.03119 ,-14.99028 ,-14.92302 ,-14.86927 ,-14.82691 ,-14.7972 ,-14.76909 ,-14.71356 ,-14.61277 ,-14.51696 ,-14.42252 ,-14.36405 ,-14.30451 ,-14.23161 ,-14.19851 ,-14.16633 ,-14.15649 ,-14.10504 ,-13.99518 ,-13.79562 ,-13.3996 ,-12.7767 ,-11.71208 ,-8.311879 ,-8.600912 ,-9.615928 ,-10.43595 ,-11.21292 ,-11.88333 ,-12.36243 ,-12.63706 ,-12.8818 ,-12.83066 ,-12.89103 ,-12.95666 ,-13.19763 ,-13.40598 ,-13.49113 ,-13.5546 ,-13.55639 ,-13.51915 ,-13.68284 ,-13.53289 ,-13.42107 ,-13.65519 ,-13.50713 ,-13.75251 ,-13.76715 ,-13.87408 ,-13.73109 ,-13.70412 ,-13.56073 ,-13.53488 ,-13.54895 ,-13.56228 ,-13.59408 ,-13.62047 ,-13.64198 ,-13.66109 ,-13.62669 ,-13.58297 ,-13.57387 ,-13.4739 ,-13.53063 ,-13.48348 ,-13.61047 ,-13.64716 ,-13.71546 ,-13.79184 ,-13.90614 ,-14.03098 ,-14.18205 ,-14.35881 ,-14.48419 ,-14.60172 ,-14.70591 ,-14.83362 ,-14.92122 ,-15.00622 ,-15.05122 ,-15.03119 ,-14.99028 ,-14.92302 ,-14.86927 ,-14.82691 ,-14.7972 ,-14.76909 ,-14.71356 ,-14.61277 ,-14.51696 ,-14.42252 ,-14.36405 ,-14.30451 ,-14.23161 ,-14.19851 ,-14.16633 ,-14.15649 ,-14.10504 ,-13.99518 ,-13.79562 ,-13.3996 ,-12.7767 ,-11.71208 ,-8.311879 ,-8.600912 ,-9.615928 ,-10.43595 ,-11.21292 ,-11.88333 ,-12.36243 ,-12.63706 ,-12.8818 ,-12.83066 ,-12.89103 ,-12.95666 ,-13.19763 ,-13.40598 ,-13.49113 ,-13.5546 ,-13.55639 ,-13.51915 ,-13.68284 ,-13.53289 ,-13.42107 ,-13.65519 ,-13.50713 ,-13.75251 ,-13.76715 ,-13.87408 ,-13.73109 ,-13.70412 ,-13.56073 ,-13.53488 ,-13.54895 ,-13.56228 ,-13.59408 ,-13.62047 ,-13.64198 ,-13.66109 ,-13.62669 ,-13.58297 ,-13.57387 ,-13.4739 ,-13.53063 ,-13.48348 ,-13.61047 ,-13.64716 ,-13.71546 ,-13.79184 ,-13.90614 ,-14.03098 ,-14.18205 ,-14.35881 ,-14.48419 ,-14.60172 ,-14.70591 ,-14.83362 ,-14.92122 ,-15.00622 ,-15.05122 ,-15.03119 ,-14.99028 ,-14.92302 ,-14.86927 ,-14.82691 ,-14.7972 ,-14.76909 ,-14.71356 ,-14.61277 ,-14.51696 ,-14.42252 ,-14.36405 ,-14.30451 ,-14.23161 ,-14.19851 ,-14.16633 ,-14.15649 ,-14.10504 ,-13.99518 ,-13.79562 ,-13.3996 ,-12.7767 ,-11.71208 ,-8.311879 ,-8.600912 ,-9.615928 ,-10.43595 ,-11.21292 ,-11.88333 ,-12.36243 ,-12.63706 ,-12.8818 ,-12.83066 ,-12.89103 ,-12.95666 ,-13.19763 ,-13.40598 ,-13.49113 ,-13.5546 ,-13.55639 ,-13.51915 ,-13.68284 ,-13.53289 ,-13.42107 ,-13.65519 ,-13.50713 ,-13.75251 ,-13.76715 ,-13.87408 ,-13.73109 ,-13.70412 ,-13.56073 ,-13.53488 ,-13.54895 ,-13.56228 ,-13.59408 ,-13.62047 ,-13.64198 ,-13.66109 ,-13.62669 ,-13.58297 ,-13.57387 ,-13.4739 ,-13.53063 ,-13.48348 ,-13.61047 ,-13.64716 ,-13.71546 ,-13.79184 ,-13.90614 ,-14.03098 ,-14.18205 ,-14.35881 ,-14.48419 ,-14.60172 ,-14.70591 ,-14.83362 ,-14.92122 ,-15.00622 ,-15.05122 ,-15.03119 ,-14.99028 ,-14.92302 ,-14.86927 ,-14.82691 ,-14.7972 ,-14.76909 ,-14.71356 ,-14.61277 ,-14.51696 ,-14.42252 ,-14.36405 ,-14.30451 ,-14.23161 ,-14.19851 ,-14.16633 ,-14.15649 ,-14.10504 ,-13.99518 ,-13.79562 ,-13.3996 ,-12.7767 ,-11.71208 ,-8.311879 ,-8.600912 ,-9.615928 ,-10.43595 ,-11.21292 ,-11.88333 ,-12.36243 ,-12.63706 ,-12.8818 ,-12.83066 ,-12.89103 ,-12.95666 ,-13.19763 ,-13.40598 ,-13.49113 ,-13.5546 ,-13.55639 ,-13.51915 ,-13.68284 ,-13.53289 ,-13.42107 ,-13.65519 ,-13.50713 ,-13.75251 ,-13.76715 ,-13.87408 ,-13.73109 ,-13.70412 ,-13.56073 ,-13.53488 ,-13.54895 ,-13.56228 ,-13.59408 ,-13.62047 ,-13.64198 ,-13.66109 ,-13.62669 ,-13.58297 ,-13.57387 ,-13.4739 ,-13.53063 ,-13.48348 ,-13.61047 ,-13.64716 ,-13.71546 ,-13.79184 ,-13.90614 ,-14.03098 ,-14.18205 ,-14.35881 ,-14.48419 ,-14.60172 ,-14.70591 ,-14.83362 ,-14.92122 ,-15.00622 ,-15.05122 ,-15.03119 ,-14.99028 ,-14.92302 ,-14.86927 ,-14.82691 ,-14.7972 ,-14.76909 ,-14.71356 ,-14.61277 ,-14.51696 ,-14.42252 ,-14.36405 ,-14.30451 ,-14.23161 ,-14.19851 ,-14.16633 ,-14.15649 ,-14.10504 ,-13.99518 ,-13.79562 ,-13.3996 ,-12.7767 ,-11.71208 ,-8.311879 ,-8.600912 ,-9.615928 ,-10.43595 ,-11.21292 ,-11.88333 ,-12.36243 ,-12.63706 ,-12.8818 ,-12.83066 ,-12.89103 ,-12.95666 ,-13.19763 ,-13.40598 ,-13.49113 ,-13.5546 ,-13.55639 ,-13.51915 ,-13.68284 ,-13.53289 ,-13.42107 ,-13.65519 ,-13.50713 ,-13.75251 ,-13.76715 ,-13.87408 ,-13.73109 ,-13.70412 ,-13.56073 ,-13.53488 ,-13.54895 ,-13.56228 ,-13.59408 ,-13.62047 ,-13.64198 ,-13.66109 ,-13.62669 ,-13.58297 ,-13.57387 ,-13.4739 ,-13.53063 ,-13.48348 ,-13.61047 ,-13.64716 ,-13.71546 ,-13.79184 ,-13.90614 ,-14.03098 ,-14.18205 ,-14.35881 ,-14.48419 ,-14.60172 ,-14.70591 ,-14.83362 ,-14.92122 ,-15.00622 ,-15.05122 ,-15.03119 ,-14.99028 ,-14.92302 ,-14.86927 ,-14.82691 ,-14.7972 ,-14.76909 ,-14.71356 ,-14.61277 ,-14.51696 ,-14.42252 ,-14.36405 ,-14.30451 ,-14.23161 ,-14.19851 ,-14.16633 ,-14.15649 ,-14.10504 ,-13.99518 ,-13.79562 ,-13.3996 ,-12.7767 ,-11.71208 ,-8.311879 ,-8.600912 ,-9.615928 ,-10.43595 ,-11.21292 ,-11.88333 ,-12.36243 ,-12.63706 ,-12.8818 ,-12.83066 ,-12.89103 ,-12.95666 ,-13.19763 ,-13.40598 ,-13.49113 ,-13.5546 ,-13.55639 ,-13.51915 ,-13.68284 ,-13.53289 ,-13.42107 ,-13.65519 ,-13.50713 ,-13.75251 ,-13.76715 ,-13.87408 ,-13.73109 ,-13.70412 ,-13.56073 ,-13.53488 ,-13.54895 ,-13.56228 ,-13.59408 ,-13.62047 ,-13.64198 ,-13.66109 ,-13.62669 ,-13.58297 ,-13.57387 ,-13.4739 ,-13.53063 ,-13.48348 ,-13.61047 ,-13.64716 ,-13.71546 ,-13.79184 ,-13.90614 ,-14.03098 ,-14.18205 ,-14.35881 ,-14.48419 ,-14.60172 ,-14.70591 ,-14.83362 ,-14.92122 ,-15.00622 ,-15.05122 ,-15.03119 ,-14.99028 ,-14.92302 ,-14.86927 ,-14.82691 ,-14.7972 ,-14.76909 ,-14.71356 ,-14.61277 ,-14.51696 ,-14.42252 ,-14.36405 ,-14.30451 ,-14.23161 ,-14.19851 ,-14.16633 ,-14.15649 ,-14.10504 ,-13.99518 ,-13.79562 ,-13.3996 ,-12.7767 ,-11.71208], dtype=np.float64)
		vars = np.array([0.155775 ,0.154484 ,0.1527379 ,0.1518718 ,0.1506028 ,0.1489256 ,0.147067 ,0.1447061 ,0.1436307 ,0.1443568 ,0.1451849 ,0.1455157 ,0.1452821 ,0.1445717 ,0.1439195 ,0.1435867 ,0.1436018 ,0.1438781 ,0.1442086 ,0.1448844 ,0.1454756 ,0.145663 ,0.146268 ,0.1467386 ,0.1472724 ,0.147664 ,0.1480913 ,0.1483739 ,0.1488841 ,0.1493636 ,0.1497088 ,0.1500379 ,0.1502916 ,0.1505389 ,0.1506787 ,0.1507102 ,0.1505992 ,0.1505445 ,0.1505938 ,0.1508133 ,0.1509569 ,0.1512396 ,0.1514625 ,0.1516195 ,0.1516156 ,0.1515561 ,0.1514966 ,0.1513976 ,0.1512612 ,0.151076 ,0.1510596 ,0.1510431 ,0.151077 ,0.1511168 ,0.1511917 ,0.151023 ,0.1508045 ,0.1505885 ,0.1503493 ,0.1502373 ,0.1501726 ,0.1500762 ,0.1500065 ,0.1499782 ,0.150057 ,0.1502658 ,0.150469 ,0.1505335 ,0.1505505 ,0.1505328 ,0.1504275 ,0.1502438 ,0.1499674 ,0.1497118 ,0.1494661 ,0.1493102 ,0.1493681 ,0.1495501 ,0.1499738 ,0.1509654 ,0.155775 ,0.154484 ,0.1527379 ,0.1518718 ,0.1506028 ,0.1489256 ,0.147067 ,0.1447061 ,0.1436307 ,0.1443568 ,0.1451849 ,0.1455157 ,0.1452821 ,0.1445717 ,0.1439195 ,0.1435867 ,0.1436018 ,0.1438781 ,0.1442086 ,0.1448844 ,0.1454756 ,0.145663 ,0.146268 ,0.1467386 ,0.1472724 ,0.147664 ,0.1480913 ,0.1483739 ,0.1488841 ,0.1493636 ,0.1497088 ,0.1500379 ,0.1502916 ,0.1505389 ,0.1506787 ,0.1507102 ,0.1505992 ,0.1505445 ,0.1505938 ,0.1508133 ,0.1509569 ,0.1512396 ,0.1514625 ,0.1516195 ,0.1516156 ,0.1515561 ,0.1514966 ,0.1513976 ,0.1512612 ,0.151076 ,0.1510596 ,0.1510431 ,0.151077 ,0.1511168 ,0.1511917 ,0.151023 ,0.1508045 ,0.1505885 ,0.1503493 ,0.1502373 ,0.1501726 ,0.1500762 ,0.1500065 ,0.1499782 ,0.150057 ,0.1502658 ,0.150469 ,0.1505335 ,0.1505505 ,0.1505328 ,0.1504275 ,0.1502438 ,0.1499674 ,0.1497118 ,0.1494661 ,0.1493102 ,0.1493681 ,0.1495501 ,0.1499738 ,0.1509654 ,0.155775 ,0.154484 ,0.1527379 ,0.1518718 ,0.1506028 ,0.1489256 ,0.147067 ,0.1447061 ,0.1436307 ,0.1443568 ,0.1451849 ,0.1455157 ,0.1452821 ,0.1445717 ,0.1439195 ,0.1435867 ,0.1436018 ,0.1438781 ,0.1442086 ,0.1448844 ,0.1454756 ,0.145663 ,0.146268 ,0.1467386 ,0.1472724 ,0.147664 ,0.1480913 ,0.1483739 ,0.1488841 ,0.1493636 ,0.1497088 ,0.1500379 ,0.1502916 ,0.1505389 ,0.1506787 ,0.1507102 ,0.1505992 ,0.1505445 ,0.1505938 ,0.1508133 ,0.1509569 ,0.1512396 ,0.1514625 ,0.1516195 ,0.1516156 ,0.1515561 ,0.1514966 ,0.1513976 ,0.1512612 ,0.151076 ,0.1510596 ,0.1510431 ,0.151077 ,0.1511168 ,0.1511917 ,0.151023 ,0.1508045 ,0.1505885 ,0.1503493 ,0.1502373 ,0.1501726 ,0.1500762 ,0.1500065 ,0.1499782 ,0.150057 ,0.1502658 ,0.150469 ,0.1505335 ,0.1505505 ,0.1505328 ,0.1504275 ,0.1502438 ,0.1499674 ,0.1497118 ,0.1494661 ,0.1493102 ,0.1493681 ,0.1495501 ,0.1499738 ,0.1509654 ,0.155775 ,0.154484 ,0.1527379 ,0.1518718 ,0.1506028 ,0.1489256 ,0.147067 ,0.1447061 ,0.1436307 ,0.1443568 ,0.1451849 ,0.1455157 ,0.1452821 ,0.1445717 ,0.1439195 ,0.1435867 ,0.1436018 ,0.1438781 ,0.1442086 ,0.1448844 ,0.1454756 ,0.145663 ,0.146268 ,0.1467386 ,0.1472724 ,0.147664 ,0.1480913 ,0.1483739 ,0.1488841 ,0.1493636 ,0.1497088 ,0.1500379 ,0.1502916 ,0.1505389 ,0.1506787 ,0.1507102 ,0.1505992 ,0.1505445 ,0.1505938 ,0.1508133 ,0.1509569 ,0.1512396 ,0.1514625 ,0.1516195 ,0.1516156 ,0.1515561 ,0.1514966 ,0.1513976 ,0.1512612 ,0.151076 ,0.1510596 ,0.1510431 ,0.151077 ,0.1511168 ,0.1511917 ,0.151023 ,0.1508045 ,0.1505885 ,0.1503493 ,0.1502373 ,0.1501726 ,0.1500762 ,0.1500065 ,0.1499782 ,0.150057 ,0.1502658 ,0.150469 ,0.1505335 ,0.1505505 ,0.1505328 ,0.1504275 ,0.1502438 ,0.1499674 ,0.1497118 ,0.1494661 ,0.1493102 ,0.1493681 ,0.1495501 ,0.1499738 ,0.1509654 ,0.155775 ,0.154484 ,0.1527379 ,0.1518718 ,0.1506028 ,0.1489256 ,0.147067 ,0.1447061 ,0.1436307 ,0.1443568 ,0.1451849 ,0.1455157 ,0.1452821 ,0.1445717 ,0.1439195 ,0.1435867 ,0.1436018 ,0.1438781 ,0.1442086 ,0.1448844 ,0.1454756 ,0.145663 ,0.146268 ,0.1467386 ,0.1472724 ,0.147664 ,0.1480913 ,0.1483739 ,0.1488841 ,0.1493636 ,0.1497088 ,0.1500379 ,0.1502916 ,0.1505389 ,0.1506787 ,0.1507102 ,0.1505992 ,0.1505445 ,0.1505938 ,0.1508133 ,0.1509569 ,0.1512396 ,0.1514625 ,0.1516195 ,0.1516156 ,0.1515561 ,0.1514966 ,0.1513976 ,0.1512612 ,0.151076 ,0.1510596 ,0.1510431 ,0.151077 ,0.1511168 ,0.1511917 ,0.151023 ,0.1508045 ,0.1505885 ,0.1503493 ,0.1502373 ,0.1501726 ,0.1500762 ,0.1500065 ,0.1499782 ,0.150057 ,0.1502658 ,0.150469 ,0.1505335 ,0.1505505 ,0.1505328 ,0.1504275 ,0.1502438 ,0.1499674 ,0.1497118 ,0.1494661 ,0.1493102 ,0.1493681 ,0.1495501 ,0.1499738 ,0.1509654 ,0.155775 ,0.154484 ,0.1527379 ,0.1518718 ,0.1506028 ,0.1489256 ,0.147067 ,0.1447061 ,0.1436307 ,0.1443568 ,0.1451849 ,0.1455157 ,0.1452821 ,0.1445717 ,0.1439195 ,0.1435867 ,0.1436018 ,0.1438781 ,0.1442086 ,0.1448844 ,0.1454756 ,0.145663 ,0.146268 ,0.1467386 ,0.1472724 ,0.147664 ,0.1480913 ,0.1483739 ,0.1488841 ,0.1493636 ,0.1497088 ,0.1500379 ,0.1502916 ,0.1505389 ,0.1506787 ,0.1507102 ,0.1505992 ,0.1505445 ,0.1505938 ,0.1508133 ,0.1509569 ,0.1512396 ,0.1514625 ,0.1516195 ,0.1516156 ,0.1515561 ,0.1514966 ,0.1513976 ,0.1512612 ,0.151076 ,0.1510596 ,0.1510431 ,0.151077 ,0.1511168 ,0.1511917 ,0.151023 ,0.1508045 ,0.1505885 ,0.1503493 ,0.1502373 ,0.1501726 ,0.1500762 ,0.1500065 ,0.1499782 ,0.150057 ,0.1502658 ,0.150469 ,0.1505335 ,0.1505505 ,0.1505328 ,0.1504275 ,0.1502438 ,0.1499674 ,0.1497118 ,0.1494661 ,0.1493102 ,0.1493681 ,0.1495501 ,0.1499738 ,0.1509654 ,0.155775 ,0.154484 ,0.1527379 ,0.1518718 ,0.1506028 ,0.1489256 ,0.147067 ,0.1447061 ,0.1436307 ,0.1443568 ,0.1451849 ,0.1455157 ,0.1452821 ,0.1445717 ,0.1439195 ,0.1435867 ,0.1436018 ,0.1438781 ,0.1442086 ,0.1448844 ,0.1454756 ,0.145663 ,0.146268 ,0.1467386 ,0.1472724 ,0.147664 ,0.1480913 ,0.1483739 ,0.1488841 ,0.1493636 ,0.1497088 ,0.1500379 ,0.1502916 ,0.1505389 ,0.1506787 ,0.1507102 ,0.1505992 ,0.1505445 ,0.1505938 ,0.1508133 ,0.1509569 ,0.1512396 ,0.1514625 ,0.1516195 ,0.1516156 ,0.1515561 ,0.1514966 ,0.1513976 ,0.1512612 ,0.151076 ,0.1510596 ,0.1510431 ,0.151077 ,0.1511168 ,0.1511917 ,0.151023 ,0.1508045 ,0.1505885 ,0.1503493 ,0.1502373 ,0.1501726 ,0.1500762 ,0.1500065 ,0.1499782 ,0.150057 ,0.1502658 ,0.150469 ,0.1505335 ,0.1505505 ,0.1505328 ,0.1504275 ,0.1502438 ,0.1499674 ,0.1497118 ,0.1494661 ,0.1493102 ,0.1493681 ,0.1495501 ,0.1499738 ,0.1509654], dtype=np.float64)
		means = np.tile(means, (frame, 1))
		vars = np.tile(vars, (frame, 1))
		inputs = (inputs + means) * vars
		return inputs

	def _get_lid(self, lid):
		if lid in list(self.lid_dict.keys()):
			return self.lid_dict[lid]
		else:
			raise ValueError(
				f"The language {lid} is not in {list(self.lid_dict.keys())}"
			)
			
	def _get_tnid(self, tnid):
		if tnid in list(self.textnorm_dict.keys()):
			return self.textnorm_dict[tnid]
		else:
			raise ValueError(
				f"The textnorm {tnid} is not in {list(self.textnorm_dict.keys())}"
			)
	
	def read_tags(self, language_input, textnorm_input):
		language_list = [self._get_lid(language_input)]
		textnorm_list = [self._get_tnid(textnorm_input)]
		return language_list, textnorm_list

	def __call__(self, wav_content: Union[str, np.ndarray, List[str]], **kwargs):
		import ailia
		self.model = ailia.Net(weight="sensevoice_small.onnx", env_id=1, memory_mode=11)

		from ailia_tokenizer import LlamaTokenizer
		self.tokenizer = LlamaTokenizer.from_pretrained("./tokenizer")

		self.batch_size = 1
		self.blank_id = 0
		self.lid_dict = {"auto": 0, "zh": 3, "en": 4, "yue": 7, "ja": 11, "ko": 12, "nospeech": 13}
		self.lid_int_dict = {24884: 3, 24885: 4, 24888: 7, 24892: 11, 24896: 12, 24992: 13}
		self.textnorm_dict = {"withitn": 14, "woitn": 15} # テキストの正規化を行うかどうか
		self.textnorm_int_dict = {25016: 14, 25017: 15}

		language_input = kwargs.get("language", "auto")
		textnorm_input = kwargs.get("textnorm", "woitn")
		language_list, textnorm_list = self.read_tags(language_input, textnorm_input)
		
		waveform_list = [wav_content]
		waveform_nums = len(waveform_list)
		
		asr_res = []
		for beg_idx in range(0, waveform_nums, self.batch_size):
			end_idx = min(waveform_nums, beg_idx + self.batch_size)
			feats, feats_len = self.extract_feat(waveform_list[beg_idx:end_idx])
			_language_list = language_list[beg_idx:end_idx]
			_textnorm_list = textnorm_list[beg_idx:end_idx]
			if not len(_language_list):
				_language_list = [language_list[0]]
				_textnorm_list = [textnorm_list[0]]
			B = feats.shape[0]
			if len(_language_list) == 1 and B != 1:
				_language_list = _language_list * B
			if len(_textnorm_list) == 1 and B != 1:
				_textnorm_list = _textnorm_list * B
			ctc_logits, encoder_out_lens = self.infer(
				feats,
				feats_len,
				np.array(_language_list, dtype=np.int32),
				np.array(_textnorm_list, dtype=np.int32),
			)
			for b in range(feats.shape[0]):
				x = ctc_logits[b, : encoder_out_lens[b].item(), :]
				yseq = np.argmax(x, axis=-1)
				# Use np.diff and np.where instead of torch.unique_consecutive.
				mask = np.concatenate(([True], np.diff(yseq) != 0))
				yseq = yseq[mask]

				mask = yseq != self.blank_id
				token_int = yseq[mask].tolist()

				text = self.tokenizer.decode(token_int, skip_special_tokens=True)
				asr_res.append(text)

		return asr_res

	def extract_feat(self, waveform_list: List[np.ndarray]) -> Tuple[np.ndarray, np.ndarray]:
		feats, feats_len = [], []
		for waveform in waveform_list:
			speech, _ = self.fbank(waveform)

			if speech is None or speech.size == 0:
				print("detected speech size {speech.size}")
				raise ValueError("Empty speech detected, skipping this waveform.")
			feat, feat_len = self.lfr_cmvn(speech)
			feats.append(feat)
			feats_len.append(feat_len)

		feats = self.pad_feats(feats, np.max(feats_len))
		feats_len = np.array(feats_len).astype(np.int32)
		return feats, feats_len

	@staticmethod
	def pad_feats(feats: List[np.ndarray], max_feat_len: int) -> np.ndarray:
		def pad_feat(feat: np.ndarray, cur_len: int) -> np.ndarray:
			pad_width = ((0, max_feat_len - cur_len), (0, 0))
			return np.pad(feat, pad_width, "constant", constant_values=0)

		feat_res = [pad_feat(feat, feat.shape[0]) for feat in feats]
		feats = np.array(feat_res).astype(np.float32)
		return feats

	def infer(
		self,
		feats: np.ndarray,
		feats_len: np.ndarray,
		language: np.ndarray,
		textnorm: np.ndarray,
	) -> Tuple[np.ndarray, np.ndarray]:
		outputs = self.model.run([feats, feats_len, language, textnorm])
		return outputs


def main():
	speech, sample_rate = soundfile.read("ja.wav")
	if speech.ndim > 1:
		speech = np.mean(speech, axis=1)
	target_sr = 16000
	if sample_rate != target_sr:
		speech = librosa.resample(speech, orig_sr=sample_rate, target_sr=target_sr)

	wav_or_scp = speech
	decoder = SimpleDecoder()
	res = decoder(wav_or_scp, language="auto", use_itn=True) # 16khz
	print(res)

if __name__ == "__main__":
	main()
