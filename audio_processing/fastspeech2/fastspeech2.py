import ailia
import numpy as np
import yaml
import sys
import matplotlib.pyplot as plt
import os
import scipy.io.wavfile as wavfile
from g2p_en import G2p
from text import text_to_sequence
import re

# utilsã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã®ãŸã‚ã«ãƒ‘ã‚¹ã‚’è¿½åŠ 
sys.path.append(".")

# ===========================
# 1. è¨­å®š
# ===========================

#ã€€ä»Šå¾Œpaeser_argsã§å¤–éƒ¨ã‹ã‚‰æŒ‡å®šã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹
FS2_ONNX_PATH = "./onnx/fastspeech2/ljspeech.onnx"
HIFI_ONNX_PATH = "./onnx/hifigan/hifigan.onnx"
PREPROCESS_CONFIG = "config/LJSpeech/preprocess.yaml"
OUTPUT_DIR = "onnx/result/LJSpeech"

# â˜…é‡è¦: ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆæ™‚ã¨åŒã˜æœ€å¤§é•· (VRAMä¸è¶³å›é¿ã®ãŸã‚ 600 ã§çµ±ä¸€)
MODEL_MAX_LENGTH = 600

# éŸ³å£°é€”åˆ‡ã‚Œã‚’é˜²ããŸã‚ã®ãƒãƒƒãƒ•ã‚¡ (10ãƒ•ãƒ¬ãƒ¼ãƒ  â‰ˆ 0.1ç§’)
MEL_BUFFER_FRAMES = 40

TEXT_TO_SPEAK = "Ailia SDK makes it easy to deploy deep learning models. This script handles both single and multi speaker models automatically."
#TEXT_TO_SPEAK = "æ—©ä¸Šå¥½ã€‚æˆ‘ä»¬ç›®å‰æ­£åœ¨å¯¹æœºå™¨å­¦ä¹ æ¨¡å‹è¿›è¡Œä¿®æ”¹ã€‚"

SPEAKER_ID = 0

# ===========================
# 2. å‰å‡¦ç†
# ===========================
def preprocess_english(text, preprocess_config):
    g2p = G2p()
    phones = []
    words = re.split(r"([,;.\-\?\!\s+])", text)
    for w in words:
        if w not in [" ", ""]:
            phones += list(filter(lambda p: p != " ", g2p(w)))
    phones = "{" + "}{".join(phones) + "}"
    phones = re.sub(r"\{[^\w\s]?\}", "{sp}", phones)
    phones = phones.replace("}{", " ")
    
    print(f"Phonemes: {phones}")
    
    sequence = np.array(
        text_to_sequence(
            phones, preprocess_config["preprocessing"]["text"]["text_cleaners"]
        )
    )
    return sequence

# ===========================
# 3. æ¨è«–ãƒ¡ã‚¤ãƒ³
# ===========================
def infer():
    if not os.path.exists(FS2_ONNX_PATH) or not os.path.exists(HIFI_ONNX_PATH):
        print("Error: ONNX file not found.")
        return

    print("Loading Config...")
    preprocess_config = yaml.load(open(PREPROCESS_CONFIG, "r"), Loader=yaml.FullLoader)
    
    print("Loading ONNX Models...")
    env_id = ailia.get_gpu_environment_id()
    
    fs2_net = ailia.Net(None, FS2_ONNX_PATH, env_id=env_id)
    hifi_net = ailia.Net(None, HIFI_ONNX_PATH, env_id=env_id)

    # -------------------------------------------
    # å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ï¼ˆãƒ‘ãƒ‡ã‚£ãƒ³ã‚°å‡¦ç†ï¼‰
    # -------------------------------------------
    print(f"Input Text: {TEXT_TO_SPEAK}")
    sequence = preprocess_english(TEXT_TO_SPEAK, preprocess_config)
    real_len = len(sequence)
    print(f"Original Length: {real_len}")
    
    # 1. ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°å‡¦ç†: å¸¸ã« max_length ã«æƒãˆã‚‹
    if real_len > MODEL_MAX_LENGTH:
        print(f"Warning: Text too long ({real_len}). Truncating to {MODEL_MAX_LENGTH}.")
        real_len = MODEL_MAX_LENGTH # Safety limit

    padded_sequence = np.zeros((1, MODEL_MAX_LENGTH), dtype=np.int64)
    padded_sequence[0, :real_len] = sequence[:real_len]

    # å…¥åŠ›å¤‰æ•°
    texts = padded_sequence
    src_lens = np.array([real_len], dtype=np.int64)
    max_src_len = np.array([MODEL_MAX_LENGTH], dtype=np.int64)
    speakers = np.array([SPEAKER_ID], dtype=np.int64)
    p_control = np.array(1.0, dtype=np.float32)
    e_control = np.array(1.0, dtype=np.float32)
    d_control = np.array(1.0, dtype=np.float32)

    # -------------------------------------------
    # FastSpeech2 æ¨è«– (Smart Input & Shape Setting)
    # -------------------------------------------
    print("Running FastSpeech2...")

    # AILIAã«ã‚·ã‚§ã‚¤ãƒ—ã‚’é€šçŸ¥ (ã‚¨ãƒ©ãƒ¼å›é¿)
    try:
        # textsã®å½¢çŠ¶ã‚’å®Ÿã‚µã‚¤ã‚ºã§é€šçŸ¥ã—ã€å†…éƒ¨ãƒãƒƒãƒ•ã‚¡ã‚’èª¿æ•´ã•ã›ã‚‹
        fs2_net.set_input_shape(fs2_net.find_blob_index_by_name("texts"), texts.shape)
        fs2_net.set_input_shape(fs2_net.find_blob_index_by_name("src_lens"), (1,))
        fs2_net.set_input_shape(fs2_net.find_blob_index_by_name("max_src_len"), (1,))
    except: pass
    
    inputs = {}
    inputs["texts"] = texts
    inputs["src_lens"] = src_lens
    inputs["max_src_len"] = max_src_len
    inputs["p_control"] = p_control
    inputs["d_control"] = d_control

    # ã‚ªãƒ—ã‚·ãƒ§ãƒ³å…¥åŠ›ã®ãƒã‚§ãƒƒã‚¯ (å­˜åœ¨ã™ã‚Œã°è¿½åŠ )
    try:
        if fs2_net.find_blob_index_by_name("speakers") != -1:
            inputs["speakers"] = speakers
    except: pass
    
    try:
        if fs2_net.find_blob_index_by_name("e_control") != -1:
            inputs["e_control"] = e_control
    except: pass

    # æ¨è«–å®Ÿè¡Œ
    fs2_res = fs2_net.predict(inputs)
    
    # -------------------------------------------
    # çµæœã®åˆ‡ã‚Šå‡ºã— (æ­£ç¢ºãªãƒˆãƒªãƒŸãƒ³ã‚°)
    # -------------------------------------------
    mel_output_padded = fs2_res[1]
    d_rounded = fs2_res[5] # å„æ–‡å­—ã®é•·ã• (d_rounded)

    # 1. æœ‰åŠ¹ãªæ–‡å­—æ•°åˆ†ã®é•·ã•ï¼ˆdurations for real textï¼‰ã‚’åˆè¨ˆ
    valid_durations = d_rounded[0, :real_len]
    
    # 2. åˆè¨ˆãƒ•ãƒ¬ãƒ¼ãƒ æ•°ã‚’è¨ˆç®—ã—ã€ãƒãƒƒãƒ•ã‚¡ã‚’è¿½åŠ  (é€”åˆ‡ã‚Œé˜²æ­¢)
    # [ä¿®æ­£ãƒã‚¤ãƒ³ãƒˆ]
    valid_mel_len = int(np.sum(valid_durations)) + MEL_BUFFER_FRAMES
    
    print(f"Calculated Mel Length (with buffer): {valid_mel_len}")

    # 3. æœ‰åŠ¹ãªéƒ¨åˆ†ã ã‘ã‚¹ãƒ‘ãƒƒã¨åˆ‡ã‚Šè½ã¨ã™
    mel_output = mel_output_padded[:, :valid_mel_len, :]
    
    # -------------------------------------------
    # HiFi-GAN æ¨è«–
    # -------------------------------------------
    print("Running HiFi-GAN...")
    
    mel_input = mel_output.transpose(0, 2, 1)
    
    # HiFi-GANã®ã‚·ã‚§ã‚¤ãƒ—ã‚‚é€šçŸ¥ (å®‰å®šæ€§å‘ä¸Š)
    try:
        hifi_net.set_input_shape(hifi_net.find_blob_index_by_name("mel_input"), mel_input.shape)
    except: pass

    audio_res = hifi_net.predict([mel_input])
    wav = audio_res[0].squeeze()

    # -------------------------------------------
    # ä¿å­˜
    # -------------------------------------------
    MAX_WAV_VALUE = 32768.0
    wav = wav * MAX_WAV_VALUE
    wav = wav.astype('int16')
    
    if not os.path.exists(OUTPUT_DIR):
        os.makedirs(OUTPUT_DIR)

    wav_path = os.path.join(OUTPUT_DIR, "output.wav")
    sampling_rate = preprocess_config["preprocessing"]["audio"]["sampling_rate"]
    wavfile.write(wav_path, sampling_rate, wav)
    print(f"ğŸ‰ Saved Audio: {wav_path}")

    plot_path = os.path.join(OUTPUT_DIR, "output_mel.png")
    plt.figure(figsize=(10, 4))
    plt.imshow(mel_output[0].T, aspect="auto", origin="lower")
    plt.title(f"Generated Mel (Len: {valid_mel_len})")
    plt.colorbar()
    plt.tight_layout()
    plt.savefig(plot_path)
    plt.close()
    print(f"ğŸ‰ Saved Plot: {plot_path}")

if __name__ == "__main__":
    infer()