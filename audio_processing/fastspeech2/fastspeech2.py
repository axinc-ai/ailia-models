import ailia
import numpy as np
import yaml
import sys
import matplotlib.pyplot as plt
import os
import scipy.io.wavfile as wavfile
from g2p_en import G2p
from pypinyin import pinyin, Style
from text import text_to_sequence
import re

# ===========================
# Settings
# ===========================

# ãƒªãƒã‚¸ãƒˆãƒªã®ãƒ«ãƒ¼ãƒˆã«ã‚ã‚‹utilsã‚’å‚ç…§ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹
sys.path.append('../../util')
from arg_utils import get_base_parser, update_parser, get_savepath  # noqa
from model_utils import check_and_download_models  # noqa

# ãƒ¢ãƒ‡ãƒ«è¨­å®š
WEIGHT_PATH_FS2 = './onnx/fastspeech2/ljspeech.onnx'
MODEL_PATH_FS2 = None
WEIGHT_PATH_HIFI = './onnx/hifigan/hifigan.onnx'
MODEL_PATH_HIFI = None
REMOTE_PATH = ""

PREPROCESS_CONFIG = "config/LJSpeech/preprocess.yaml"

# â˜…é‡è¦: ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆæ™‚ã¨åŒã˜æœ€å¤§é•· (VRAMä¸è¶³å›žé¿ã®ãŸã‚ 600 ã§çµ±ä¸€)
MODEL_MAX_LENGTH = 600

# éŸ³å£°é€”åˆ‡ã‚Œã‚’é˜²ããŸã‚ã®ãƒãƒƒãƒ•ã‚¡ (10ãƒ•ãƒ¬ãƒ¼ãƒ  â‰ˆ 0.1ç§’)
MEL_BUFFER_FRAMES = 40

# ===========================
# Arguments
# ===========================
parser = get_base_parser(
    'FastSpeech2',
    'fastspeech2.py',
    'output.wav'
)
# å…ƒã®FastSpeech2ãƒªãƒã‚¸ãƒˆãƒªã¨åŒã˜å¼•æ•°å
parser.add_argument(
    '--source',
    type=str,
    default=None,
    help='path to a source file with format like train.txt and val.txt'
)
parser.add_argument(
    '--restore_step',
    type=int,
    required=False,
    default=900000,
    help='step for checkpoint to restore'
)
parser.add_argument(
    '--mode',
    type=str,
    choices=['batch', 'single'],
    required=False,
    default='single',
    help='Synthesize a whole dataset or a single sentence'
)
parser.add_argument(
    '-t', '--text',
    type=str,
    default="Ailia SDK makes it easy to deploy deep learning models.",
    help='raw text to synthesize, for single-sentence mode only'
)
parser.add_argument(
    '--speaker_id',
    type=int,
    default=0,
    help='speaker ID for multi-speaker synthesis, for single-sentence mode only'
)
parser.add_argument(
    '-p', '--pitch_control',
    type=float,
    default=1.0,
    help='control the pitch of the whole utterance, larger value for higher pitch'
)
parser.add_argument(
    '-e', '--energy_control',
    type=float,
    default=1.0,
    help='control the energy of the whole utterance, larger value for larger volume'
)
parser.add_argument(
    '-d', '--duration_control',
    type=float,
    default=1.0,
    help='control the speed of the whole utterance, larger value for slower speaking rate'
)
# ailiaå›ºæœ‰ã®å¼•æ•°
parser.add_argument(
    '--preprocess_config',
    type=str,
    default=PREPROCESS_CONFIG,
    help='path to preprocess.yaml'
)
parser.add_argument(
    '--model_config',
    type=str,
    default='config/LJSpeech/model.yaml',
    help='path to model.yaml'
)
parser.add_argument(
    '--onnx_fs2',
    default=WEIGHT_PATH_FS2,
    help='Path to FastSpeech2 ONNX file.'
)
parser.add_argument(
    '--onnx_hifi',
    default=WEIGHT_PATH_HIFI,
    help='Path to HiFi-GAN ONNX file.'
)
args = update_parser(parser)


# ===========================
# 2. å‰å‡¦ç†(è‹±èªžã¨ä¸­å›½èªžã§ç•°ãªã‚‹)
# ===========================
def preprocess_english(text, preprocess_config):
    g2p = G2p()
    phones = []
    words = re.split(r"([,;.\-\?\!\s+])", text)
    for w in words:
        if w not in [" ", ""]:
            phones += list(filter(lambda p: p != " ", g2p(w)))
    phones = "{" + "}{".join(phones) + "}"
    phones = re.sub(r"\{[^\w\s]?\}", "{sp}", phones)
    phones = phones.replace("}{", " ")
    
    print(f"Phonemes: {phones}")
    
    sequence = np.array(
        text_to_sequence(
            phones, preprocess_config["preprocessing"]["text"]["text_cleaners"]
        )
    )
    return sequence

def preprocess_mandarin(text, preprocess_config):
    lexicon = read_lexicon(preprocess_config["path"]["lexicon_path"])

    phones = []
    pinyins = [
        p[0]
        for p in pinyin(
            text, style=Style.TONE3, strict=False, neutral_tone_with_five=True
        )
    ]
    for p in pinyins:
        if p in lexicon:
            phones += lexicon[p]
        else:
            phones.append("sp")

    phones = "{" + " ".join(phones) + "}"
    print(f"Phonemes: {phones}")

    sequence = np.array(
        text_to_sequence(
            phones, preprocess_config["preprocessing"]["text"]["text_cleaners"]
        )
    )
    return sequence

def read_lexicon(lex_path):
    lexicon = {}
    with open(lex_path) as f:
        for line in f:
            temp = re.split(r"\s+", line.strip("\n"))
            word = temp[0]
            phones = temp[1:]
            if word.lower() not in lexicon:
                lexicon[word.lower()] = phones
    return lexicon

def get_preprocess_method(preprocess_config):
    dataset = preprocess_config["dataset"]
    if dataset == "LJSpeech":
        return preprocess_english
    if dataset == "LibriTTS":
        return preprocess_english
    if dataset == "AISHELL3":
        return preprocess_mandarin
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯è‹±èªžã¨ã™ã‚‹
    return preprocess_english

# ===========================
# 3. Main Inference
# ===========================
def infer():
    # ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    check_and_download_models(args.onnx_fs2, MODEL_PATH_FS2, REMOTE_PATH)
    check_and_download_models(args.onnx_hifi, MODEL_PATH_HIFI, REMOTE_PATH)

    print("Loading Config...")
    # preprocess_configã‚’èª­ã¿è¾¼ã¿
    preprocess_config = yaml.load(open(args.preprocess_config, "r"), Loader=yaml.FullLoader)
    
    print("Loading ONNX Models...")
    env_id = args.env_id
    
    # ailia.Netã®åˆæœŸåŒ–
    fs2_net = ailia.Net(MODEL_PATH_FS2, args.onnx_fs2, env_id=env_id)
    hifi_net = ailia.Net(MODEL_PATH_HIFI, args.onnx_hifi, env_id=env_id)

    # -------------------------------------------
    # å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ï¼ˆãƒ‘ãƒ‡ã‚£ãƒ³ã‚°å‡¦ç†ï¼‰
    # -------------------------------------------
    # sourceãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Œã°ãã“ã‹ã‚‰èª­ã¿è¾¼ã¿ã€ãªã‘ã‚Œã°textã‚’ä½¿ç”¨
    texts_to_process = []
    if hasattr(args, 'source') and args.source and os.path.exists(args.source):
        print(f"Reading texts from source file: {args.source}")
        with open(args.source, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line:
                    # ãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆ: speaker_id|text ã‚‚ã—ãã¯ text ã®ã¿
                    if '|' in line:
                        parts = line.split('|')
                        if len(parts) >= 2:
                            texts_to_process.append((parts[0], '|'.join(parts[1:])))
                        else:
                            texts_to_process.append((str(args.speaker_id), line))
                    else:
                        texts_to_process.append((str(args.speaker_id), line))
    else:
        # ã‚³ãƒžãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã‹ã‚‰ã®å˜ä¸€ãƒ†ã‚­ã‚¹ãƒˆ
        texts_to_process.append((str(args.speaker_id), args.text))
    
    # å„ãƒ†ã‚­ã‚¹ãƒˆã‚’å‡¦ç†
    for idx, (speaker_str, text) in enumerate(texts_to_process):
        speaker_id = int(speaker_str) if speaker_str.isdigit() else args.speaker_id
        
        print(f"\n{'='*60}")
        print(f"Processing text {idx+1}/{len(texts_to_process)}")
        print(f"Speaker ID: {speaker_id}")
        print(f"Input Text: {text}")
        
        preprocess_func = get_preprocess_method(preprocess_config)
        sequence = preprocess_func(text, preprocess_config)
        
        real_len = len(sequence)
        print(f"Original Length: {real_len}")
        
        # 1. ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°å‡¦ç†: å¸¸ã« max_length ã«æƒãˆã‚‹
        if real_len > MODEL_MAX_LENGTH:
            print(f"Warning: Text too long ({real_len}). Truncating to {MODEL_MAX_LENGTH}.")
            real_len = MODEL_MAX_LENGTH # Safety limit

        padded_sequence = np.zeros((1, MODEL_MAX_LENGTH), dtype=np.int64)
        padded_sequence[0, :real_len] = sequence[:real_len]

        # å…¥åŠ›å¤‰æ•°ï¼ˆå¼•æ•°ã‹ã‚‰åˆ¶å¾¡ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–å¾—ï¼‰
        texts = padded_sequence
        src_lens = np.array([real_len], dtype=np.int64)
        max_src_len = np.array([MODEL_MAX_LENGTH], dtype=np.int64)
        speakers = np.array([speaker_id], dtype=np.int64)
        p_control = np.array(args.pitch_control, dtype=np.float32)
        e_control = np.array(args.energy_control, dtype=np.float32)
        d_control = np.array(args.duration_control, dtype=np.float32)

        # FastSpeech2æŽ¨è«–ã¨HiFi-GANå‡¦ç†ã‚’å®Ÿè¡Œ
        _synthesize(fs2_net, hifi_net, texts, src_lens, max_src_len, speakers, 
                   p_control, e_control, d_control, preprocess_config, sequence, real_len, idx)

def _synthesize(fs2_net, hifi_net, texts, src_lens, max_src_len, speakers, 
               p_control, e_control, d_control, preprocess_config, sequence, real_len, idx=0):

    # -------------------------------------------
    # FastSpeech2 æŽ¨è«– (Smart Input & Shape Setting)
    # -------------------------------------------
    print("Running FastSpeech2...")

    # AILIAã«ã‚·ã‚§ã‚¤ãƒ—ã‚’é€šçŸ¥ (ã‚¨ãƒ©ãƒ¼å›žé¿)
    try:
        fs2_net.set_input_shape(fs2_net.find_blob_index_by_name("texts"), texts.shape)
        fs2_net.set_input_shape(fs2_net.find_blob_index_by_name("src_lens"), (1,))
        fs2_net.set_input_shape(fs2_net.find_blob_index_by_name("max_src_len"), (1,))
    except: pass
    
    inputs = {}
    inputs["texts"] = texts
    inputs["src_lens"] = src_lens
    inputs["max_src_len"] = max_src_len
    inputs["p_control"] = p_control
    inputs["d_control"] = d_control

    # ã‚ªãƒ—ã‚·ãƒ§ãƒ³å…¥åŠ›ã®ãƒã‚§ãƒƒã‚¯
    try:
        if fs2_net.find_blob_index_by_name("speakers") != -1:
            inputs["speakers"] = speakers
    except: pass
    
    try:
        if fs2_net.find_blob_index_by_name("e_control") != -1:
            inputs["e_control"] = e_control
    except: pass

    # æŽ¨è«–å®Ÿè¡Œ
    fs2_res = fs2_net.predict(inputs)
    
    
    # -------------------------------------------
    # çµæžœã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ— (spãƒŽã‚¤ã‚ºé™¤åŽ»ã¨æ­£ç¢ºãªåˆ‡ã‚Šå‡ºã—)
    # -------------------------------------------
    mel_output_padded = fs2_res[1]
    d_rounded = fs2_res[5] # å„æ–‡å­—ã®é•·ã•

    # --- [Step 1] sp (ç„¡éŸ³) åŒºé–“ã®å®Œå…¨ãƒŸãƒ¥ãƒ¼ãƒˆå‡¦ç† ---
    # sp ã® ID ã‚’ç‰¹å®š
    cleaner_name = preprocess_config["preprocessing"]["text"]["text_cleaners"]
    sp_id_seq = text_to_sequence("{sp}", cleaner_name)
    sp_id = sp_id_seq[0]

    # Melå…¨ä½“ã®æœ€å°å€¤ï¼ˆ=ç„¡éŸ³ãƒ¬ãƒ™ãƒ«ï¼‰ã‚’å–å¾—
    min_mel_val = np.min(mel_output_padded)

    current_frame = 0
    # å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã®å„ãƒˆãƒ¼ã‚¯ãƒ³ã‚’èµ°æŸ»ã—ã€spãªã‚‰å¡—ã‚Šã¤ã¶ã™
    for i in range(real_len):
        dur = int(d_rounded[0, i])
        token_id = sequence[i]

        if token_id == sp_id:
            # spåŒºé–“ã‚’æœ€å°å€¤ã§ä¸Šæ›¸ã (ãƒŽã‚¤ã‚ºé™¤åŽ»)
            mel_output_padded[0, current_frame : current_frame + dur, :] = min_mel_val
        
        current_frame += dur

    # --- [Step 2] æ­£ç¢ºãªåˆ‡ã‚Šå‡ºã—ã¨ãƒãƒƒãƒ•ã‚¡å‡¦ç† ---
    # æœ‰åŠ¹ãªéŸ³å£°é•·ã‚’è¨ˆç®—
    valid_durations = d_rounded[0, :real_len]
    valid_mel_len = int(np.sum(valid_durations))
    print(f"Calculated Mel Length (Content): {valid_mel_len}")

    # ã‚´ãƒŸã‚’å«ã¾ãªã„ã‚ˆã†ã€æœ‰åŠ¹éƒ¨åˆ†ã ã‘ã‚’åˆ‡ã‚Šå‡ºã™
    mel_output_clean = mel_output_padded[:, :valid_mel_len, :]
    
    # ãƒãƒƒãƒ•ã‚¡ï¼ˆä½™éŸ»ï¼‰ãŒå¿…è¦ãªå ´åˆã€ç„¡éŸ³ï¼ˆæœ€å°å€¤ï¼‰ã‚’è¿½åŠ 
    if MEL_BUFFER_FRAMES > 0:
        silence_padding = np.full(
            (1, MEL_BUFFER_FRAMES, mel_output_clean.shape[2]),
            min_mel_val,
            dtype=mel_output_clean.dtype
        )
        mel_output = np.concatenate([mel_output_clean, silence_padding], axis=1)
        print(f"Added {MEL_BUFFER_FRAMES} frames of silence padding.")
    else:
        mel_output = mel_output_clean
    
    # -------------------------------------------
    # HiFi-GAN æŽ¨è«–
    # -------------------------------------------
    print("Running HiFi-GAN...")
    
    mel_input = np.ascontiguousarray(mel_output.transpose(0, 2, 1))
    
    try:
        hifi_net.set_input_shape(hifi_net.find_blob_index_by_name("mel_input"), mel_input.shape)
    except: pass

    audio_res = hifi_net.predict([mel_input])
    wav = audio_res[0].squeeze()

    # -------------------------------------------
    # ä¿å­˜
    # -------------------------------------------
    MAX_WAV_VALUE = 32768.0
    wav = wav * MAX_WAV_VALUE
    wav = wav.astype('int16')
    
    # è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«å¯¾å¿œ: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½¿ã£ãŸä¿å­˜ãƒ‘ã‚¹ç”Ÿæˆ
    if idx > 0:
        base, ext = os.path.splitext(args.savepath)
        savepath = f"{base}_{idx}{ext}"
    else:
        savepath = args.savepath
    
    print(f"Saving to {savepath}")
    
    sampling_rate = preprocess_config["preprocessing"]["audio"]["sampling_rate"]
    wavfile.write(savepath, sampling_rate, wav)
    print(f"ðŸŽ‰ Saved Audio: {savepath}")

    # Plot saving
    plot_path = savepath.replace(".wav", "_mel.png")
    plt.figure(figsize=(10, 4))
    plt.imshow(mel_output[0].T, aspect="auto", origin="lower")
    plt.title(f"Generated Mel (Len: {mel_output.shape[1]})")
    plt.colorbar()
    plt.tight_layout()
    plt.savefig(plot_path)
    plt.close()
    print(f"ðŸŽ‰ Saved Plot: {plot_path}")

if __name__ == "__main__":
    infer()