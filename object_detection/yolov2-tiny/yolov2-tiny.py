import sys
import time

import numpy as np
import cv2

import ailia

# import original modules
sys.path.append('../../util')
from utils import get_base_parser, update_parser, get_savepath  # noqa: E402
from model_utils import check_and_download_models  # noqa: E402
from detector_utils import plot_results, load_image  # noqa: E402
import webcamera_utils  # noqa: E402

# logger
from logging import getLogger   # noqa: E402
logger = getLogger(__name__)

from yolo_utils import *

# ======================
# Parameters
# ======================
REMOTE_PATH = 'https://storage.googleapis.com/ailia-models/yolov2-tiny/'
IMAGE_PATH = 'input.jpg'
SAVE_IMAGE_PATH = 'output.png'
IMAGE_HEIGHT = 416  # for video mode
IMAGE_WIDTH = 416  # for video mode


COCO_CATEGORY = [
    "person", "bicycle", "car", "motorcycle", "airplane", "bus", "train",
    "truck", "boat", "traffic light", "fire hydrant", "stop sign",
    "parking meter", "bench", "bird", "cat", "dog", "horse", "sheep", "cow",
    "elephant", "bear", "zebra", "giraffe", "backpack", "umbrella",
    "handbag", "tie", "suitcase", "frisbee", "skis", "snowboard",
    "sports ball", "kite", "baseball bat", "baseball glove", "skateboard",
    "surfboard", "tennis racket", "bottle", "wine glass", "cup", "fork",
    "knife", "spoon", "bowl", "banana", "apple", "sandwich", "orange",
    "broccoli", "carrot", "hot dog", "pizza", "donut", "cake", "chair",
    "couch", "potted plant", "bed", "dining table", "toilet", "tv",
    "laptop", "mouse", "remote", "keyboard", "cell phone", "microwave",
    "oven", "toaster", "sink", "refrigerator", "book", "clock", "vase",
    "scissors", "teddy bear", "hair drier", "toothbrush"
]

VOC_CATEGORY = [
    "aeroplane","bicycle","bird","boat","bottle",
    "bus","car","cat","chair","cow","diningtable",
    "dog","horse","motorbike","person","pottedplant",
    "sheep","sofa","train","tvmonitor"
]


COCO_ANCHORS = np.array(
    [0.57273, 0.677385, 1.87446, 2.06253, 3.33843,
     5.47434, 7.88282, 3.52778, 9.77052, 9.16828]
)

VOC_ANCHORS = np.array(
        [1.08,1.19,  3.42,4.41,  6.63,
         11.38,  9.42,5.11,  16.62,10.52]
)

CATEGORY = COCO_CATEGORY
ANCHORS = COCO_ANCHORS

THRESHOLD = 0.4
IOU = 0.45



# ======================
# Arguemnt Parser Config
# ======================
parser = get_base_parser('Yolov2 tiny model', IMAGE_PATH, SAVE_IMAGE_PATH)
parser.add_argument(
    '-dt', '--detector',
    action='store_true',
    help='Use detector API (require ailia SDK 1.2.7).'
)

parser.add_argument(
    '-dataset', '--dataset',
    metavar='DATASET', default='coco'
)
args = update_parser(parser)

if args.dataset=='voc':
    ANCHORS  = VOC_ANCHORS
    CATEGORY = VOC_CATEGORY
    WEIGHT_PATH = 'yolov2-tiny-voc.onnx'
    MODEL_PATH  = 'yolov2-tiny-voc.onnx.prototxt'
elif args.dataset=='coco':
    ANCHORS  = COCO_ANCHORS
    CATEGORY = COCO_CATEGORY
    WEIGHT_PATH = 'yolov2-tiny-coco.onnx'
    MODEL_PATH  = 'yolov2-tiny-coco.onnx.prototxt'

def detect(img,output,savepath='output.png', conf_thresh=0.5, nms_thresh=0.4,video=False):
    num_classes = len(CATEGORY) 
    
    if num_classes == 20:
        namesfile = 'voc.names'
    elif num_classes == 80:
        namesfile = 'coco.names'
    else:
        namesfile = 'data/names'

    for i in range(2):
        boxes = get_region_boxes(output, conf_thresh, num_classes, ANCHORS, 5)[0]
        #boxes = get_region_boxes(output, conf_thresh, num_classes, anchors, num_anchors)[0]
        boxes = nms(boxes, nms_thresh)

    class_names = load_class_names(namesfile)
    if video:
        return plot_boxes(img, boxes, False, class_names)
    else:
        return plot_boxes(img, boxes, savepath, class_names)

# ======================
# Main functions
# ======================
def recognize_from_image():
    # net initialize
    if args.detector == True:
        detector = ailia.Detector(
            MODEL_PATH,
            WEIGHT_PATH,
            len(CATEGORY),
            format=ailia.NETWORK_IMAGE_FORMAT_RGB,
            channel=ailia.NETWORK_IMAGE_CHANNEL_FIRST,
            range=ailia.NETWORK_IMAGE_RANGE_S_FP32,
            algorithm=ailia.DETECTOR_ALGORITHM_YOLOV2,
            env_id=args.env_id,
        )
        detector.set_anchors(ANCHORS)
        if args.profile:
            detector.set_profile_mode(True)
    else:
        print("path",WEIGHT_PATH)
        net = ailia.Net(None,WEIGHT_PATH)
    
    # input image loop
    for image_path in args.input:
        # prepare input data logger.info(image_path)
        img = load_image(image_path)
        logger.debug(f'input image shape: {img.shape}')

        # inference
        logger.info('Start inference...')
        if args.benchmark:
            logger.info('BENCHMARK mode')
            for i in range(5):
                start = int(round(time.time() * 1000))
                if args.detector:
                    detector.compute(img, THRESHOLD, IOU)
                else:
                    pass
                end = int(round(time.time() * 1000))
                logger.info(f'\tailia processing time {end - start} ms')
            # plot result
            res_img = plot_results(detector, img, CATEGORY)
            savepath = get_savepath(args.savepath, image_path)
            logger.info(f'saved at : {savepath}')
            cv2.imwrite(savepath, res_img)
            if args.profile:
                print(detector.get_summary())
 
        else:
            if args.detector:
                detector.compute(img, THRESHOLD, IOU)
                # plot result
                res_img = plot_results(detector, img, CATEGORY)
                savepath = get_savepath(args.savepath, image_path)
                logger.info(f'saved at : {savepath}')
                cv2.imwrite(savepath, res_img)
                if args.profile:
                    print(detector.get_summary())
 
            else:
                savepath = get_savepath(args.savepath, image_path)

                img_PIL = Image.open(image_path).convert('RGB')
                input_data = cv2.imread(image_path)
                input_data = cv2.resize(input_data, (416,416))/ 255
                input_data = input_data.transpose((2,0,1))
                input_data = input_data[np.newaxis,:,:,:].astype(np.float32)
                results = net.run([input_data])
                results = torch.FloatTensor(results[0])
                detect(img_PIL,results,savepath,video=False)

    logger.info('Script finished successfully.')


def recognize_from_video():
    # net initialize
    if args.detector == True:
        detector = ailia.Detector(
            MODEL_PATH,
            WEIGHT_PATH,
            len(CATEGORY),
            format=ailia.NETWORK_IMAGE_FORMAT_RGB,
            channel=ailia.NETWORK_IMAGE_CHANNEL_FIRST,
            range=ailia.NETWORK_IMAGE_RANGE_S_FP32,
            algorithm=ailia.DETECTOR_ALGORITHM_YOLOV2,
            env_id=args.env_id,
        )
        detector.set_anchors(ANCHORS)
    else:
        net = ailia.Net(None,WEIGHT_PATH)

    capture = webcamera_utils.get_capture(args.video)

    # create video writer if savepath is specified as video format
    if args.savepath != SAVE_IMAGE_PATH:
        f_h = int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT))
        f_w = int(capture.get(cv2.CAP_PROP_FRAME_WIDTH))
        writer = webcamera_utils.get_writer(args.savepath, f_h, f_w)
    else:
        writer = None

    while(True):
        ret, frame = capture.read()
        if (cv2.waitKey(1) & 0xFF == ord('q')) or not ret:
            break 
        if args.detector:
            img = cv2.cvtColor(frame, cv2.COLOR_BGR2BGRA)
            detector.compute(img, THRESHOLD, IOU)
            res_img = plot_results(detector, frame, CATEGORY, False)
        else:
            img_PIL = Image.fromarray(frame)
            img = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
            img = cv2.resize(img, (416,416))
            img = img.transpose((2,0,1))/255
            img = img[np.newaxis,:,:,:].astype(np.float32)
            results = net.run([img])
            results = torch.FloatTensor(results[0])
            output_img = detect(img_PIL,results, video=True)
            res_img = np.array(output_img,dtype=np.uint8)

        cv2.imshow('frame', res_img)

        # save results
        if writer is not None:
            writer.write(res_img)

    capture.release()
    cv2.destroyAllWindows()
    if writer is not None:
        writer.release()
    logger.info('Script finished successfully.')


def main():
    # model files check and download
    check_and_download_models(WEIGHT_PATH, MODEL_PATH, REMOTE_PATH)
    
    if args.video is not None:
        # video mode
        recognize_from_video()
    else:
        # image mode
        recognize_from_image()


if __name__ == '__main__':
    main()
