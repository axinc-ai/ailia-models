import os
from typing import TypedDict, Optional, List
from PIL import Image
import numpy as np
from skimage import io, img_as_float32, transform
import scipy.io as scio

class AnimeGeneratorInput(TypedDict):
    source_image: np.ndarray
    source_semantics: np.ndarray
    target_semantics: np.ndarray
    frame_num: int
    video_name: str
    audio_path: str


class AnimeGenerator:
    def __init__(self):
        # self.generator = 
        pass

    def preprocess(
        self,
        first_coeff_path: str,
        coeff_path: str,
        pic_path: str,
        audio_path: str,
        size: int,
        batch_size: int,
        preprocess: str,
        expression_scale: float,
        still: bool,
    ) -> AnimeGeneratorInput:
        """
        Build input data for AnimeGenerator

        Args:
            first_coeff_path (str): path to coefficient file (Generated by ImageProcessor.generate)
            coeff_path (str): path to the coefficient file (Generated by AudioProcessor.generate)
            pic_path (str): path to image file processed by ImageProcessor.generate
            audio_path (str): path to audio file
            size (int): size of the input image
            batch_size (int): batch size
            preprocess (str): preprocess
            expression_scale (float): 
            still (bool): 
        Returns:
            data (dict): input data for AnimeGenerator
        """
        semantic_radius = 13
        video_name = os.path.splitext(os.path.split(coeff_path)[-1])[0]
        txt_path = os.path.splitext(coeff_path)[0]

        data ={}
        img1 = Image.open(pic_path)
        source_image = np.array(img1)
        source_image = img_as_float32(source_image)
        source_image = transform.resize(source_image, (size, size, 3))
        source_image = source_image.transpose((2, 0, 1))
        source_image_ts = np.expand_dims(source_image, axis=0)
        source_image_ts = np.tile(source_image_ts, (batch_size, 1, 1, 1))
        data['source_image'] = source_image_ts

        source_semantics_dict = scio.loadmat(first_coeff_path)
        if 'full' not in preprocess.lower():
            source_semantics = source_semantics_dict['coeff_3dmm'][:1,:70]         #1 70
        else:
            source_semantics = source_semantics_dict['coeff_3dmm'][:1,:73]         #1 70
        source_semantics_new = self._transform_semantic_1(source_semantics, semantic_radius)
        source_semantics_ts = np.expand_dims(source_semantics_new, axis=0)
        source_semantics_ts = np.tile(source_semantics_ts, (batch_size, 1, 1))
        data['source_semantics'] = source_semantics_ts

        generated_dict = scio.loadmat(coeff_path)
        generated_3dmm = generated_dict['coeff_3dmm']
        generated_3dmm[:, :64] = generated_3dmm[:, :64] * expression_scale

        if 'full' in preprocess.lower():
            generated_3dmm = np.concatenate([generated_3dmm, np.repeat(source_semantics[:,70:], generated_3dmm.shape[0], axis=0)], axis=1)

        if still:
            generated_3dmm[:, 64:] = np.repeat(source_semantics[:, 64:], generated_3dmm.shape[0], axis=0)

        with open(txt_path+'.txt', 'w') as f:
            for coeff in generated_3dmm:
                for i in coeff:
                    f.write(str(i)[:7]   + '  '+'\t')
                f.write('\n')

        frame_num = generated_3dmm.shape[0]
        data['frame_num'] = frame_num

        target_semantics_list = [] 
        for frame_idx in range(frame_num):
            target_semantics = self._transform_semantic_target(generated_3dmm, frame_idx, semantic_radius)
            target_semantics_list.append(target_semantics)

        remainder = frame_num%batch_size
        if remainder!=0:
            for _ in range(batch_size-remainder):
                target_semantics_list.append(target_semantics)

        target_semantics_np = np.array(target_semantics_list)             #frame_num 70 semantic_radius*2+1
        target_semantics_np = target_semantics_np.reshape(batch_size, -1, target_semantics_np.shape[-2], target_semantics_np.shape[-1])
        data['target_semantics'] = target_semantics_np
        data['video_name'] = video_name
        data['audio_path'] = audio_path
        return AnimeGeneratorInput(
            source_image=data["source_image"],
            source_semantics=data["source_semantics"],
            target_semantics=data["target_semantics"],
            frame_num=data["frame_num"],
            video_name=data["video_name"],
            audio_path=data["audio_path"],
        )

    def generate(
        self,
        gen_input: AnimeGeneratorInput,
    ):
        source_image = gen_input['source_image']
        source_semantics = gen_input['source_semantics']
        target_semantics = gen_input['target_semantics']
        frame_num = gen_input['frame_num']

        predictions_video = self._make_animation(
            source_image,
            source_semantics,
            target_semantics,
        )

    def _transform_semantic_1(self, semantic: np.ndarray, semantic_radius: int):
        semantic_list =  [semantic for i in range(0, semantic_radius*2+1)]
        coeff_3dmm = np.concatenate(semantic_list, 0)
        return coeff_3dmm.transpose(1,0)

    def _transform_semantic_target(self, coeff_3dmm: np.ndarray, frame_index: int, semantic_radius: int):
        num_frames = coeff_3dmm.shape[0]
        seq = list(range(frame_index- semantic_radius, frame_index + semantic_radius+1))
        index = [ min(max(item, 0), num_frames-1) for item in seq ] 
        coeff_3dmm_g = coeff_3dmm[index, :]
        return coeff_3dmm_g.transpose(1,0)

    def _make_animation(
        self,
        source_image: np.ndarray,
        source_semantics: np.ndarray,
    ):
        pass
